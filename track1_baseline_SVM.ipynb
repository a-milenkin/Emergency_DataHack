{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import (RepeatedKFold, cross_val_score,\n",
    "                              train_test_split, GridSearchCV,GridSearchCV)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam\n",
       "0  2000        3019    1      0.0\n",
       "1  2000        3019    2      0.0\n",
       "2  2000        3019    3      0.0\n",
       "3  2000        3019    4      0.0\n",
       "4  2000        3019    5      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "train = pd.read_csv('track_1/train.csv') # main_df\n",
    "print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# города\n",
    "# Киренск, посёлок городского типа Витим, посёлок городского типа Пеледуй, село Крестовский \n",
    "# Лесоуùасток, город Ленск, город Олёкминск, город Покровск, город Якутск, село Батамай, \n",
    "# посёлок городского типа Сангар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"Images/post_map.PNG\"  width=900></p>\n",
    "<h3 style=\"text-align: center;\"><b>Карта метеостанций и гидропостов</b></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузим всё\n",
    "# train = pd.read_csv('1_track_extra_train/hydro_1day.csv') # main_df\n",
    "# print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "# train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузим всё\n",
    "# train = pd.read_csv('track_1/hydro_1day.csv') # main_df\n",
    "# print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "# train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hydro_1day.csv', 'meteo_1day.csv', 'meteo_1month.csv', 'meteo_3hours.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к директории с данными\n",
    "\n",
    "data_dir_new = '1_track_extra_train/'\n",
    "os.listdir(data_dir_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hydro_1day.csv',\n",
       " 'hydro_coord.csv',\n",
       " 'ice_saw.csv',\n",
       " 'meteo_1day.csv',\n",
       " 'meteo_1month.csv',\n",
       " 'meteo_3hours.csv',\n",
       " 'meteo_coord.csv',\n",
       " 'reference_horiz_visib.csv',\n",
       " 'reference_water_codes.csv',\n",
       " 'test.csv',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к директории с данными\n",
    "\n",
    "data_dir = './track_1/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam\n",
       "0  2000        3019    1      0.0\n",
       "1  2000        3019    2      0.0\n",
       "2  2000        3019    3      0.0\n",
       "3  2000        3019    4      0.0\n",
       "4  2000        3019    5      0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "train = pd.read_csv(data_dir + 'train.csv') # main_df\n",
    "print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['year', 'station_id', 'day', 'ice_jam'], dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "test = pd.read_csv(data_dir + 'test.csv') # main_df\n",
    "print(test.day.unique())  #Дни больше чем 30 !!!!\n",
    "test.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. \n",
    "\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подтянем ближайшую к гидростанции метеостанцию\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "import re\n",
    "\n",
    "def merge_coord(df):\n",
    "    df['lat_long'] = df[['lat', 'lon']].apply(tuple, axis=1)\n",
    "    return df\n",
    "\n",
    "def stat_km(point, stat_list):\n",
    "    stations_list=stat_list\n",
    "    lst=[]\n",
    "    if pd.isnull(point):\n",
    "        lst.append(np.nan)\n",
    "    else:\n",
    "        for i in stations_list['lat_long']:\n",
    "            x=geodesic(point, i).km\n",
    "            lst.append(x)\n",
    "            stations_list['dist']=pd.DataFrame(lst)\n",
    "        y=stations_list['station_id'][stations_list['dist'] == stations_list['dist'].min()]\n",
    "        y=y.to_string()\n",
    "        y=re.sub(\"^[0-9]+\", \"\", y)\n",
    "        y=re.sub(\" +\", \"\", y)\n",
    "        return int(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(346130, 48)\n",
      "(10926, 17)\n",
      "(10926, 17)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10926 entries, 0 to 10925\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   met_station_id                  10926 non-null  int64  \n",
      " 1   lat                             10926 non-null  float64\n",
      " 2   lon                             10926 non-null  float64\n",
      " 3   z                               10926 non-null  float64\n",
      " 4   lat_long                        10926 non-null  object \n",
      " 5   dist                            10926 non-null  float64\n",
      " 6   year                            10926 non-null  int64  \n",
      " 7   month                           10926 non-null  int64  \n",
      " 8   data_qual                       8801 non-null   float64\n",
      " 9   precipitation_observed          8801 non-null   float64\n",
      " 10  precipitation_corrected         8801 non-null   float64\n",
      " 11  precipitation_corrected_liquid  8801 non-null   float64\n",
      " 12  precipitation_corrected_mixed   8801 non-null   float64\n",
      " 13  precipitation_corrected_solid   8801 non-null   float64\n",
      " 14  sunshine_hours                  7784 non-null   float64\n",
      " 15  date                            10926 non-null  object \n",
      " 16  day                             10926 non-null  int64  \n",
      "dtypes: float64(11), int64(4), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo_coord = pd.read_csv(data_dir + 'meteo_coord.csv')  # mc\n",
    "meteo_coord.drop(['name'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Гидро\n",
    "hydro_coord = pd.read_csv(data_dir + 'hydro_coord.csv') # hc\n",
    "hydro_coord.drop(columns = ['name'], inplace=True)\n",
    "\n",
    "\n",
    "hydro_coord = merge_coord(hydro_coord)\n",
    "meteo_coord = merge_coord(meteo_coord)\n",
    "hydro_coord['met_station_id'] = meteo_coord.lat_long.apply(lambda x: stat_km(x, meteo_coord))\n",
    "\n",
    "\n",
    "hydro_coord.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "meteo_coord.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "\n",
    "\n",
    "meteo_1day = pd.read_csv(data_dir_new + 'meteo_1day.csv')  #mld\n",
    "meteo_1day.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "# meteo_1day.drop(['date'], inplace=True, axis=1)\n",
    "print(meteo_1day.shape)\n",
    "# print(meteo_1day.columns)\n",
    "#meteo_1day = meteo_coord.merge(meteo_1day, on=['met_station_id'], how='left')\n",
    "\n",
    "meteo_1month = pd.read_csv(data_dir_new + 'meteo_1month.csv')  #mlm\n",
    "meteo_1month.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "meteo_1month = meteo_coord.merge(meteo_1month, on=['met_station_id'], how='left')\n",
    "#meteo_1month.drop(['date'], inplace=True, axis=1)\n",
    "# print(meteo_1month.shape)\n",
    "# print(meteo_1month.columns)\n",
    "# meteo_1month.head(2)\n",
    "\n",
    "\n",
    "print(meteo_1month.shape)\n",
    "# meteo_all = meteo_1day.merge(meteo_1month, on=['met_station_id', 'year', 'month'], \n",
    "#                 how='left')\n",
    "# meteo_all.rename(columns = {\"date_x\":\"date_1day\", \"date_y\":\"date_1month\", \"day_x\":\"day\",\"day_y\":\"day_1month\"}, inplace=True)\n",
    "\n",
    "meteo_all = meteo_1month.copy()\n",
    "#TODOs исправить\n",
    "\n",
    "\n",
    "# meteo_all.fillna(0, inpalce=True) # Закодировать наличие пропуска в толшине воды/льда\n",
    "print(meteo_all.shape)\n",
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOs добавить инфу из meteo_3hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer_horiz_visib = pd.read_csv(data_dir + 'reference_horiz_visib.csv') #rhv\n",
    "# refer_horiz_visib.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гидро"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydro_coord (27, 8)\n",
      "hydro_1day размер (234138, 16)\n",
      "Index(['year', 'hyd_station_id', 'month', 'day', 'date', 'stage_avg',\n",
      "       'stage_min', 'stage_max', 'temp', 'water_code', 'ice_thickness',\n",
      "       'snow_height', 'place', 'discharge', 'ice_thickness_is_none',\n",
      "       'snow_height_is_none'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# # Гидро\n",
    "# hydro_coord = pd.read_csv(data_dir + 'hydro_coord.csv') # hc\n",
    "# hydro_coord.drop(columns = ['name'], inplace=True)\n",
    "# hydro_coord.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "\n",
    "print('hydro_coord', hydro_coord.shape)\n",
    "#print(hydro_coord.columns)\n",
    "# hydro_coord.head(3)\n",
    "\n",
    "\n",
    "hydro_1day = pd.read_csv(data_dir_new + 'hydro_1day.csv',  parse_dates=['date'])  # hld             \n",
    "hydro_1day.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "#hydro_1day.drop('date', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def check_is_none(x):  #Заметим что фичу не надо категоризировать!\n",
    "    if x is None: return 1\n",
    "    else: return 0\n",
    "\n",
    "    \n",
    "    \n",
    "hydro_1day['ice_thickness_is_none'] =hydro_1day['ice_thickness'].apply(check_is_none)\n",
    "hydro_1day['ice_thickness'].fillna(hydro_1day['ice_thickness'].mean(), inplace=True)\n",
    "\n",
    "hydro_1day['snow_height_is_none'] =hydro_1day['snow_height'].apply(check_is_none)\n",
    "hydro_1day['snow_height'].fillna(hydro_1day['snow_height'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "print('hydro_1day размер',hydro_1day.shape)\n",
    "print(hydro_1day.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_1day = hydro_1day.groupby(['hyd_station_id', 'year', 'month'], as_index=False).agg(stage_max_mean = ('stage_max', 'mean'),\n",
    "                                                                           stage_max_min = ('stage_max', 'min'),\n",
    "                                                                           stage_max_max = ('stage_max', 'max'),\n",
    "                                                                           \n",
    "                                                                           stage_min_mean = ('stage_min', 'mean'),\n",
    "                                                                           stage_min_min = ('stage_min', 'min'),\n",
    "                                                                           stage_min_max = ('stage_min', 'max'),\n",
    "                                                                           \n",
    "                                                                           stage_avg_mean =('stage_avg', 'mean'),\n",
    "                                                                           stage_avg_min = ('stage_avg', 'min'),\n",
    "                                                                           stage_avg_max = ('stage_avg', 'max'),\n",
    "                                                                        \n",
    "                                                                           ice_thickness_mean = ('ice_thickness', 'mean'),\n",
    "                                                                           ice_thickness_min = ('ice_thickness', 'min'),\n",
    "                                                                           ice_thickness_max = ('ice_thickness', 'max'),\n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                           snow_height_mean = ('snow_height', 'mean'),\n",
    "                                                                           snow_height_min = ('snow_height', 'min'),\n",
    "                                                                           snow_height_max = ('snow_height', 'max'),\n",
    "                                                                           \n",
    "                                                                            \n",
    "                                                                           discharge_mean = ('discharge', 'mean'),\n",
    "                                                                           discharge_min = ('discharge', 'min'),\n",
    "                                                                           discharge_max = ('discharge', 'max'),\n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                          ice_thickness_none_mean = ('ice_thickness_is_none', 'mean'),\n",
    "                                                                          snow_height_none_mean = ('snow_height_is_none', 'mean')\n",
    "                                                                                        )          \n",
    "                                                                                        \n",
    "                                                                            #water_code\n",
    "                                                                                         \n",
    "                                                                \n",
    "\n",
    "# pd.Series.mode\n",
    "#.agg(lambda x:x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydro_all размер (7869, 30) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hyd_station_id           7869 non-null   int64  \n",
      " 1   lat                      7869 non-null   float64\n",
      " 2   lon                      7869 non-null   float64\n",
      " 3   distance_from_source     7869 non-null   float64\n",
      " 4   drainage_area            7869 non-null   int64  \n",
      " 5   z_null                   7869 non-null   float64\n",
      " 6   lat_long                 7869 non-null   object \n",
      " 7   met_station_id           7869 non-null   int64  \n",
      " 8   year                     7869 non-null   int64  \n",
      " 9   month                    7869 non-null   int64  \n",
      " 10  stage_max_mean           7865 non-null   float64\n",
      " 11  stage_max_min            7865 non-null   float64\n",
      " 12  stage_max_max            7865 non-null   float64\n",
      " 13  stage_min_mean           7865 non-null   float64\n",
      " 14  stage_min_min            7865 non-null   float64\n",
      " 15  stage_min_max            7865 non-null   float64\n",
      " 16  stage_avg_mean           7865 non-null   float64\n",
      " 17  stage_avg_min            7865 non-null   float64\n",
      " 18  stage_avg_max            7865 non-null   float64\n",
      " 19  ice_thickness_mean       7869 non-null   float64\n",
      " 20  ice_thickness_min        7869 non-null   float64\n",
      " 21  ice_thickness_max        7869 non-null   float64\n",
      " 22  snow_height_mean         7869 non-null   float64\n",
      " 23  snow_height_min          7869 non-null   float64\n",
      " 24  snow_height_max          7869 non-null   float64\n",
      " 25  discharge_mean           2991 non-null   float64\n",
      " 26  discharge_min            2991 non-null   float64\n",
      " 27  discharge_max            2991 non-null   float64\n",
      " 28  ice_thickness_none_mean  7869 non-null   int64  \n",
      " 29  snow_height_none_mean    7869 non-null   int64  \n",
      "dtypes: float64(22), int64(7), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "hydro_all = hydro_coord.merge(hydro_1day, on=['hyd_station_id'], how='left')\n",
    "\n",
    "print('hydro_all размер',hydro_all.shape,'\\n')\n",
    "\n",
    "# hydro_all.head(3)\n",
    "#TODOs исправить \n",
    "\n",
    "hydro_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice_saw (1547, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>place</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-04-02</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-03</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  place                                               name  \\\n",
       "0  2011-04-01  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "1  2011-04-02  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "2  2011-04-03  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "\n",
       "         lat      lon  \n",
       "0  61.378333  114.565  \n",
       "1  61.378333  114.565  \n",
       "2  61.378333  114.565  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Анастасия \n",
    "#\n",
    "ice_saw = pd.read_csv(data_dir + 'ice_saw.csv') #rhv\n",
    "#мб фича в том есть ли пробел между точкой и словом\n",
    "print('ice_saw',ice_saw.shape)\n",
    "#Почистим немного данные,чтобы привести к одному виду\n",
    "ice_saw.name = ice_saw.name.str.strip()\n",
    "ice_saw.name = ice_saw.name.str.replace('р. ', 'р.')\n",
    "ice_saw.name = ice_saw.name.str.replace('г. ', 'г.')\n",
    "ice_saw.name = ice_saw.name.str.replace('е Р', 'е, Р')\n",
    "ice_saw.name = ice_saw.name.str.replace('ики Са', 'ика Са')\n",
    "ice_saw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гео is comning\n",
    "#TODO какой гидроцентр по счету сверху \n",
    "#TODO вытащить фичи из названия участка реки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подтянем ближайшую к гидростанции метеостанцию\n",
    "\n",
    "# from geopy.distance import geodesic\n",
    "# import re\n",
    "\n",
    "# def merge_coord(df):\n",
    "#     df['lat_long'] = df[['lat', 'lon']].apply(tuple, axis=1)\n",
    "#     return df\n",
    "\n",
    "# def stat_km(point, stat_list):\n",
    "#     stations_list=stat_list\n",
    "#     lst=[]\n",
    "#     if pd.isnull(point):\n",
    "#         lst.append(np.nan)\n",
    "#     else:\n",
    "#         for i in stations_list['lat_long']:\n",
    "#             x=geodesic(point, i).km\n",
    "#             lst.append(x)\n",
    "#             stations_list['dist']=pd.DataFrame(lst)\n",
    "#         y=stations_list['met_station_id'][stations_list['dist'] == stations_list['dist'].min()]\n",
    "#         y=y.to_string()\n",
    "#         y=re.sub(\"^[0-9]+\", \"\", y)\n",
    "#         y=re.sub(\" +\", \"\", y)\n",
    "#         return int(y)\n",
    "\n",
    "\n",
    "# hydro_coord = merge_coord(hydro_coord)\n",
    "# meteo_coord = merge_coord(meteo_coord)\n",
    "# hydro_coord['closest_hydro'] = meteo_coord.lat_long.apply(lambda x: stat_km(x, meteo_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyd_station_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_source</th>\n",
       "      <th>drainage_area</th>\n",
       "      <th>z_null</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>met_station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>stage_max_mean</th>\n",
       "      <th>stage_max_min</th>\n",
       "      <th>stage_max_max</th>\n",
       "      <th>stage_min_mean</th>\n",
       "      <th>stage_min_min</th>\n",
       "      <th>stage_min_max</th>\n",
       "      <th>stage_avg_mean</th>\n",
       "      <th>stage_avg_min</th>\n",
       "      <th>stage_avg_max</th>\n",
       "      <th>ice_thickness_mean</th>\n",
       "      <th>ice_thickness_min</th>\n",
       "      <th>ice_thickness_max</th>\n",
       "      <th>snow_height_mean</th>\n",
       "      <th>snow_height_min</th>\n",
       "      <th>snow_height_max</th>\n",
       "      <th>discharge_mean</th>\n",
       "      <th>discharge_min</th>\n",
       "      <th>discharge_max</th>\n",
       "      <th>ice_thickness_none_mean</th>\n",
       "      <th>snow_height_none_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>-25.580645</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-25.709677</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-25.709677</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>71.622206</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>25.176103</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>2</td>\n",
       "      <td>-25.928571</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-26.107143</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-26.107143</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>73.083349</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>26.563003</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>3</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>75.233645</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>23.714655</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>5</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hyd_station_id    lat     lon  distance_from_source  drainage_area  z_null  \\\n",
       "0            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "1            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "2            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "3            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "4            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "\n",
       "          lat_long  met_station_id  year  month  stage_max_mean  \\\n",
       "0  (57.77, 108.07)           24538  1985      1      -25.580645   \n",
       "1  (57.77, 108.07)           24538  1985      2      -25.928571   \n",
       "2  (57.77, 108.07)           24538  1985      3      -27.516129   \n",
       "3  (57.77, 108.07)           24538  1985      4       -8.233333   \n",
       "4  (57.77, 108.07)           24538  1985      5      200.516129   \n",
       "\n",
       "   stage_max_min  stage_max_max  stage_min_mean  stage_min_min  stage_min_max  \\\n",
       "0          -28.0          -23.0      -25.709677          -28.0          -23.0   \n",
       "1          -28.0          -23.0      -26.107143          -28.0          -23.0   \n",
       "2          -32.0          -19.0      -27.516129          -32.0          -19.0   \n",
       "3          -20.0           15.0       -8.233333          -20.0           15.0   \n",
       "4            0.0          420.0      200.516129            0.0          420.0   \n",
       "\n",
       "   stage_avg_mean  stage_avg_min  stage_avg_max  ice_thickness_mean  \\\n",
       "0      -25.709677          -28.0          -23.0           71.622206   \n",
       "1      -26.107143          -28.0          -23.0           73.083349   \n",
       "2      -27.516129          -32.0          -19.0           75.233645   \n",
       "3       -8.233333          -20.0           15.0           75.651536   \n",
       "4      200.516129            0.0          420.0           75.651536   \n",
       "\n",
       "   ice_thickness_min  ice_thickness_max  snow_height_mean  snow_height_min  \\\n",
       "0          50.000000          75.651536         25.176103        22.898368   \n",
       "1          61.000000          75.651536         26.563003        22.898368   \n",
       "2          69.000000          75.651536         23.714655        22.898368   \n",
       "3          75.651536          75.651536         22.898368        22.898368   \n",
       "4          75.651536          75.651536         22.898368        22.898368   \n",
       "\n",
       "   snow_height_max  discharge_mean  discharge_min  discharge_max  \\\n",
       "0        44.000000             NaN            NaN            NaN   \n",
       "1        43.000000             NaN            NaN            NaN   \n",
       "2        35.000000             NaN            NaN            NaN   \n",
       "3        22.898368             NaN            NaN            NaN   \n",
       "4        22.898368             NaN            NaN            NaN   \n",
       "\n",
       "   ice_thickness_none_mean  snow_height_none_mean  \n",
       "0                        0                      0  \n",
       "1                        0                      0  \n",
       "2                        0                      0  \n",
       "3                        0                      0  \n",
       "4                        0                      0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydro_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hyd_station_id           7869 non-null   int64  \n",
      " 1   lat                      7869 non-null   float64\n",
      " 2   lon                      7869 non-null   float64\n",
      " 3   distance_from_source     7869 non-null   float64\n",
      " 4   drainage_area            7869 non-null   int64  \n",
      " 5   z_null                   7869 non-null   float64\n",
      " 6   lat_long                 7869 non-null   object \n",
      " 7   met_station_id           7869 non-null   int64  \n",
      " 8   year                     7869 non-null   int64  \n",
      " 9   month                    7869 non-null   int64  \n",
      " 10  stage_max_mean           7865 non-null   float64\n",
      " 11  stage_max_min            7865 non-null   float64\n",
      " 12  stage_max_max            7865 non-null   float64\n",
      " 13  stage_min_mean           7865 non-null   float64\n",
      " 14  stage_min_min            7865 non-null   float64\n",
      " 15  stage_min_max            7865 non-null   float64\n",
      " 16  stage_avg_mean           7865 non-null   float64\n",
      " 17  stage_avg_min            7865 non-null   float64\n",
      " 18  stage_avg_max            7865 non-null   float64\n",
      " 19  ice_thickness_mean       7869 non-null   float64\n",
      " 20  ice_thickness_min        7869 non-null   float64\n",
      " 21  ice_thickness_max        7869 non-null   float64\n",
      " 22  snow_height_mean         7869 non-null   float64\n",
      " 23  snow_height_min          7869 non-null   float64\n",
      " 24  snow_height_max          7869 non-null   float64\n",
      " 25  discharge_mean           2991 non-null   float64\n",
      " 26  discharge_min            2991 non-null   float64\n",
      " 27  discharge_max            2991 non-null   float64\n",
      " 28  ice_thickness_none_mean  7869 non-null   int64  \n",
      " 29  snow_height_none_mean    7869 non-null   int64  \n",
      "dtypes: float64(22), int64(7), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "hydro_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10926 entries, 0 to 10925\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   met_station_id                  10926 non-null  int64  \n",
      " 1   lat                             10926 non-null  float64\n",
      " 2   lon                             10926 non-null  float64\n",
      " 3   z                               10926 non-null  float64\n",
      " 4   lat_long                        10926 non-null  object \n",
      " 5   dist                            10926 non-null  float64\n",
      " 6   year                            10926 non-null  int64  \n",
      " 7   month                           10926 non-null  int64  \n",
      " 8   data_qual                       8801 non-null   float64\n",
      " 9   precipitation_observed          8801 non-null   float64\n",
      " 10  precipitation_corrected         8801 non-null   float64\n",
      " 11  precipitation_corrected_liquid  8801 non-null   float64\n",
      " 12  precipitation_corrected_mixed   8801 non-null   float64\n",
      " 13  precipitation_corrected_solid   8801 non-null   float64\n",
      " 14  sunshine_hours                  7784 non-null   float64\n",
      " 15  date                            10926 non-null  object \n",
      " 16  day                             10926 non-null  int64  \n",
      "dtypes: float64(11), int64(4), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert True, 'стоп'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гидро + Метео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7869, 30)\n",
      "(7869, 44)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 40 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   hyd_station_id                  7869 non-null   int64  \n",
      " 1   lat                             7869 non-null   float64\n",
      " 2   lon                             7869 non-null   float64\n",
      " 3   distance_from_source            7869 non-null   float64\n",
      " 4   drainage_area                   7869 non-null   int64  \n",
      " 5   z_null                          7869 non-null   float64\n",
      " 6   year                            7869 non-null   int64  \n",
      " 7   month                           7869 non-null   int64  \n",
      " 8   stage_max_mean                  7865 non-null   float64\n",
      " 9   stage_max_min                   7865 non-null   float64\n",
      " 10  stage_max_max                   7865 non-null   float64\n",
      " 11  stage_min_mean                  7865 non-null   float64\n",
      " 12  stage_min_min                   7865 non-null   float64\n",
      " 13  stage_min_max                   7865 non-null   float64\n",
      " 14  stage_avg_mean                  7865 non-null   float64\n",
      " 15  stage_avg_min                   7865 non-null   float64\n",
      " 16  stage_avg_max                   7865 non-null   float64\n",
      " 17  ice_thickness_mean              7869 non-null   float64\n",
      " 18  ice_thickness_min               7869 non-null   float64\n",
      " 19  ice_thickness_max               7869 non-null   float64\n",
      " 20  snow_height_mean                7869 non-null   float64\n",
      " 21  snow_height_min                 7869 non-null   float64\n",
      " 22  snow_height_max                 7869 non-null   float64\n",
      " 23  discharge_mean                  2991 non-null   float64\n",
      " 24  discharge_min                   2991 non-null   float64\n",
      " 25  discharge_max                   2991 non-null   float64\n",
      " 26  ice_thickness_none_mean         7869 non-null   int64  \n",
      " 27  snow_height_none_mean           7869 non-null   int64  \n",
      " 28  lat_met                         7264 non-null   float64\n",
      " 29  lon_met                         7264 non-null   float64\n",
      " 30  z                               7264 non-null   float64\n",
      " 31  dist                            7264 non-null   float64\n",
      " 32  data_qual                       5718 non-null   float64\n",
      " 33  precipitation_observed          5718 non-null   float64\n",
      " 34  precipitation_corrected         5718 non-null   float64\n",
      " 35  precipitation_corrected_liquid  5718 non-null   float64\n",
      " 36  precipitation_corrected_mixed   5718 non-null   float64\n",
      " 37  precipitation_corrected_solid   5718 non-null   float64\n",
      " 38  sunshine_hours                  4987 non-null   float64\n",
      " 39  day                             7264 non-null   float64\n",
      "dtypes: float64(34), int64(6)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "print(hydro_all.shape)\n",
    "DATA = hydro_all.merge(meteo_all, on = ['met_station_id','year','month'], suffixes=('','_met'), how='left') # day \n",
    "print(DATA.shape)\n",
    "#print(DATA.columns)\n",
    "\n",
    "DATA.drop(['met_station_id', 'date', 'lat_long', 'lat_long_met'], axis=1, inplace=True)\n",
    "\n",
    "#Вот эти возможно позже заюзаем\n",
    "#DATA.drop(['date_1day','date_1month'], axis=1, inplace=True)\n",
    "DATA.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# water_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_codes = hydro_1day.water_code.unique()\n",
    "\n",
    "# def get_ditct_freq(text_codes, code_freq = {}):\n",
    "#     for code in text_codes:\n",
    "#         if str(code) != 'nan':\n",
    "#             if len(code.split(','))== 1:\n",
    "#                 if code in code_freq.keys():\n",
    "#                     code_freq[code] +=1\n",
    "#                 else: code_freq[code] = 1\n",
    "#             else:\n",
    "#                 for x in code.split(','):\n",
    "#                     if x in code_freq.keys():\n",
    "#                         code_freq[x] +=1\n",
    "#                     else: code_freq[x] = 1\n",
    "                    \n",
    "#     return code_freq\n",
    "\n",
    "# water_code_freq = get_ditct_freq(text_codes)            \n",
    "    \n",
    "    \n",
    "# def get_water_code_freq_max_min(x):\n",
    "#     if x is None: return -1\n",
    "#     x = x.strip() \n",
    "#     if len(x.split(',')) == 1: return int(x), water_code_freq[i], -1\n",
    "#     x = x.strip().split(',')\n",
    "#     y = np.array([water_code_freq[i] for i in x])\n",
    "#     return x[np.argmax(y)], x[np.argmin(y)], max(y), min(y)  # Самое частое, самое редкое, частота первого, частота второго\n",
    "\n",
    "\n",
    "# def get_len_water_code(x):\n",
    "#     if x is None:\n",
    "#         return -1\n",
    "#     return len(str(x).split(','))\n",
    "    \n",
    "     \n",
    "# # water_code_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA['len_water_code'] =DATA['water_code'].apply(get_len_water_code)\n",
    "# DATA[['len_water_code', 'water_code']]\n",
    "\n",
    "\n",
    "#TODOs применить  get_water_code_freq_max_min\n",
    "# df_met_day[['latitude', 'longitude']].apply(get_hyd_id,  axis=1)\n",
    "\n",
    "#TODs применить лэйбл кода\n",
    "# refer_water_codes = pd.read_csv(data_dir + 'reference_water_codes.csv') # rwc\n",
    "# refer_water_codes #TODOs заюзать!\n",
    "\n",
    "#DATA.drop(['water_code'], axis=1, inplace=True)  #Извлекли, все что смогли, теперь дропаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 40 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   hyd_station_id                  7869 non-null   int64  \n",
      " 1   lat                             7869 non-null   float64\n",
      " 2   lon                             7869 non-null   float64\n",
      " 3   distance_from_source            7869 non-null   float64\n",
      " 4   drainage_area                   7869 non-null   int64  \n",
      " 5   z_null                          7869 non-null   float64\n",
      " 6   year                            7869 non-null   int64  \n",
      " 7   month                           7869 non-null   int64  \n",
      " 8   stage_max_mean                  7865 non-null   float64\n",
      " 9   stage_max_min                   7865 non-null   float64\n",
      " 10  stage_max_max                   7865 non-null   float64\n",
      " 11  stage_min_mean                  7865 non-null   float64\n",
      " 12  stage_min_min                   7865 non-null   float64\n",
      " 13  stage_min_max                   7865 non-null   float64\n",
      " 14  stage_avg_mean                  7865 non-null   float64\n",
      " 15  stage_avg_min                   7865 non-null   float64\n",
      " 16  stage_avg_max                   7865 non-null   float64\n",
      " 17  ice_thickness_mean              7869 non-null   float64\n",
      " 18  ice_thickness_min               7869 non-null   float64\n",
      " 19  ice_thickness_max               7869 non-null   float64\n",
      " 20  snow_height_mean                7869 non-null   float64\n",
      " 21  snow_height_min                 7869 non-null   float64\n",
      " 22  snow_height_max                 7869 non-null   float64\n",
      " 23  discharge_mean                  2991 non-null   float64\n",
      " 24  discharge_min                   2991 non-null   float64\n",
      " 25  discharge_max                   2991 non-null   float64\n",
      " 26  ice_thickness_none_mean         7869 non-null   int64  \n",
      " 27  snow_height_none_mean           7869 non-null   int64  \n",
      " 28  lat_met                         7264 non-null   float64\n",
      " 29  lon_met                         7264 non-null   float64\n",
      " 30  z                               7264 non-null   float64\n",
      " 31  dist                            7264 non-null   float64\n",
      " 32  data_qual                       5718 non-null   float64\n",
      " 33  precipitation_observed          5718 non-null   float64\n",
      " 34  precipitation_corrected         5718 non-null   float64\n",
      " 35  precipitation_corrected_liquid  5718 non-null   float64\n",
      " 36  precipitation_corrected_mixed   5718 non-null   float64\n",
      " 37  precipitation_corrected_solid   5718 non-null   float64\n",
      " 38  sunshine_hours                  4987 non-null   float64\n",
      " 39  day                             7264 non-null   float64\n",
      "dtypes: float64(34), int64(6)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_is_none(x):  #Заметим что фичу не надо категоризировать!\n",
    "#     if x is None: return 1\n",
    "#     else: return 0\n",
    "\n",
    "    \n",
    "    \n",
    "# DATA['ice_thickness_is_none'] =DATA['ice_thickness'].apply(check_is_none)\n",
    "# DATA['ice_thickness'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['snow_height_is_none'] =DATA['snow_height'].apply(check_is_none)\n",
    "# DATA['snow_height'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_aver_is_none'] =DATA['ice_crust_aver'].apply(check_is_none)\n",
    "# DATA['ice_crust_aver'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_route_is_none'] =DATA['ice_crust_route'].apply(check_is_none)\n",
    "# DATA['ice_crust_route'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_route_is_none'] =DATA['ice_crust_route'].apply(check_is_none)\n",
    "# DATA['ice_crust_route'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['snow_saturated_thickness_is_none'] =DATA['snow_saturated_thickness'].apply(check_is_none)\n",
    "# DATA['snow_saturated_thickness'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# ice_thickness\n",
    "# snow_height\n",
    "# snow_saturated_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заполняем средними численные значения\n",
    "\n",
    "sunshine_hours_mean = DATA.sunshine_hours.mean()\n",
    "DATA.sunshine_hours.fillna(sunshine_hours_mean, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#TODOs сделать поумнее заполнение нулей\n",
    "#TODOs разобраться почему у нас наны вообще,мб не везде верный мерджинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns\n",
    "\n",
    "common_columns = ['lat', 'lon', 'z_null'] \n",
    "month_columns = ['stage_max_mean', 'stage_max_min',\n",
    "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
    "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
    "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
    "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
    "       'discharge_mean', 'discharge_min', 'discharge_max',\n",
    "       'ice_thickness_none_mean', 'snow_height_none_mean',\n",
    "       'data_qual', 'precipitation_observed',\n",
    "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
    "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
    "       'sunshine_hours']\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>hyd_station_id</th>\n",
       "      <th>day_for_pred</th>\n",
       "      <th>ice_jam</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  hyd_station_id  day_for_pred  ice_jam  M1  M2  M3  M4\n",
       "0  2000            3019             1      0.0   1   2   3   4\n",
       "1  2000            3019             2      0.0   1   2   3   4\n",
       "2  2000            3019             3      0.0   1   2   3   4\n",
       "3  2000            3019             4      0.0   1   2   3   4\n",
       "4  2000            3019             5      0.0   1   2   3   4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_month_key(day):  # Избегаем только месяцев после заторного периода\n",
    "    if day <= 9: return 1, 2, 3, 4\n",
    "    elif day > 9 and day < 39: return 2, 3, 4, 5\n",
    "    else: return 3, 4, 5, 6\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "tmp = train['day'].apply(get_month_key)\n",
    "train['M1'] = tmp.apply(lambda x: x[0])\n",
    "train['M2'] = tmp.apply(lambda x: x[1])\n",
    "train['M3'] = tmp.apply(lambda x: x[2])\n",
    "train['M4'] = tmp.apply(lambda x: x[3])\n",
    "\n",
    "\n",
    "\n",
    "tmp = test['day'].apply(get_month_key)\n",
    "test['M1'] = tmp.apply(lambda x: x[0])\n",
    "test['M2'] = tmp.apply(lambda x: x[1])\n",
    "test['M3'] = tmp.apply(lambda x: x[2])\n",
    "test['M4'] = tmp.apply(lambda x: x[3])\n",
    "\n",
    "\n",
    "#train.head()\n",
    "train.rename(columns = {'station_id':'hyd_station_id', 'day' : 'day_for_pred'}, inplace=True)\n",
    "test.rename(columns = {'station_id':'hyd_station_id', 'day' : 'day_for_pred'}, inplace=True)\n",
    "print(train.columns)\n",
    "train.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hyd_station_id', 'lat', 'lon', 'distance_from_source', 'drainage_area',\n",
       "       'z_null', 'year', 'month', 'stage_max_mean', 'stage_max_min',\n",
       "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
       "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
       "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
       "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
       "       'discharge_mean', 'discharge_min', 'discharge_max',\n",
       "       'ice_thickness_none_mean', 'snow_height_none_mean', 'lat_met',\n",
       "       'lon_met', 'z', 'dist', 'data_qual', 'precipitation_observed',\n",
       "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
       "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
       "       'sunshine_hours', 'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA to TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11277, 8)\n",
      "(7869, 44)\n",
      "(11277, 49)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'ice_thickness_none_mean', 'snow_height_none_mean',\n",
      "       'lat_met', 'lon_met', 'z', 'dist', 'data_qual',\n",
      "       'precipitation_observed', 'precipitation_corrected',\n",
      "       'precipitation_corrected_liquid', 'precipitation_corrected_mixed',\n",
      "       'precipitation_corrected_solid', 'sunshine_hours', 'day', 'M2_M1',\n",
      "       'M3_M1', 'M4_M1'],\n",
      "      dtype='object')\n",
      "M1\n",
      "(11277, 76)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'ice_thickness_none_mean', 'snow_height_none_mean',\n",
      "       'lat_met', 'lon_met', 'z', 'dist', 'data_qual',\n",
      "       'precipitation_observed', 'precipitation_corrected',\n",
      "       'precipitation_corrected_liquid', 'precipitation_corrected_mixed',\n",
      "       'precipitation_corrected_solid', 'sunshine_hours', 'day', 'M2_M1',\n",
      "       'M3_M1', 'M4_M1', 'stage_max_mean_M2', 'stage_max_min_M2',\n",
      "       'stage_max_max_M2', 'stage_min_mean_M2', 'stage_min_min_M2',\n",
      "       'stage_min_max_M2', 'stage_avg_mean_M2', 'stage_avg_min_M2',\n",
      "       'stage_avg_max_M2', 'ice_thickness_mean_M2', 'ice_thickness_min_M2',\n",
      "       'ice_thickness_max_M2', 'snow_height_mean_M2', 'snow_height_min_M2',\n",
      "       'snow_height_max_M2', 'discharge_mean_M2', 'discharge_min_M2',\n",
      "       'discharge_max_M2', 'ice_thickness_none_mean_M2',\n",
      "       'snow_height_none_mean_M2', 'data_qual_M2', 'precipitation_observed_M2',\n",
      "       'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2',\n",
      "       'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2',\n",
      "       'sunshine_hours_M2'],\n",
      "      dtype='object')\n",
      "M2\n",
      "(11277, 104)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'ice_thickness_none_mean_M3', 'snow_height_none_mean_M3',\n",
      "       'data_qual_M3', 'precipitation_observed_M3',\n",
      "       'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3',\n",
      "       'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3',\n",
      "       'sunshine_hours_M3', 'M2_M3'],\n",
      "      dtype='object', length=104)\n",
      "(11277, 133)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'snow_height_none_mean_M4', 'data_qual_M4', 'precipitation_observed_M4',\n",
      "       'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4',\n",
      "       'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4',\n",
      "       'sunshine_hours_M4', 'M2_M4', 'M3_M4'],\n",
      "      dtype='object', length=133)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11277 entries, 0 to 11276\n",
      "Columns: 133 entries, year to M3_M4\n",
      "dtypes: float64(122), int64(11)\n",
      "memory usage: 11.5 MB\n"
     ]
    }
   ],
   "source": [
    "on = ['year','hyd_station_id', 'M1'] \n",
    "\n",
    "#DATA.rename(columns = {'month':'M1'}, inplace=True)\n",
    "\n",
    "DATA['M1'] = DATA['month']\n",
    "DATA['M2'] = DATA['month']\n",
    "DATA['M3'] = DATA['month']\n",
    "DATA['M4'] = DATA['month']\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(DATA.shape)\n",
    "DATA_train = train.merge(DATA, on = ['year','hyd_station_id', 'M1'], suffixes=('','_M1'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "print('M1')\n",
    "\n",
    "\n",
    "# # DATA.rename(columns = {'M1':'M2'}, inplace=True)\n",
    "month_columns +=  ['year','hyd_station_id' , 'M2']\n",
    "# month_columns\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M2'], suffixes=('','_M2'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "print('M2')\n",
    "month_columns +=  ['M3']\n",
    "# DATA.rename(columns = {'M2':'M3'}, inplace=True)\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M3'], suffixes=('','_M3'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "month_columns +=  [ 'M4']\n",
    "# DATA.rename(columns = {'M3':'M4'}, inplace=True)\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M4'], suffixes=('','_M4'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "\n",
    "DATA_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA to TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3906, 8)\n",
      "(7869, 44)\n",
      "(3906, 49)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'ice_thickness_none_mean', 'snow_height_none_mean',\n",
      "       'lat_met', 'lon_met', 'z', 'dist', 'data_qual',\n",
      "       'precipitation_observed', 'precipitation_corrected',\n",
      "       'precipitation_corrected_liquid', 'precipitation_corrected_mixed',\n",
      "       'precipitation_corrected_solid', 'sunshine_hours', 'day', 'M2_M1',\n",
      "       'M3_M1', 'M4_M1'],\n",
      "      dtype='object')\n",
      "M1\n",
      "(3906, 76)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'ice_thickness_none_mean', 'snow_height_none_mean',\n",
      "       'lat_met', 'lon_met', 'z', 'dist', 'data_qual',\n",
      "       'precipitation_observed', 'precipitation_corrected',\n",
      "       'precipitation_corrected_liquid', 'precipitation_corrected_mixed',\n",
      "       'precipitation_corrected_solid', 'sunshine_hours', 'day', 'M2_M1',\n",
      "       'M3_M1', 'M4_M1', 'stage_max_mean_M2', 'stage_max_min_M2',\n",
      "       'stage_max_max_M2', 'stage_min_mean_M2', 'stage_min_min_M2',\n",
      "       'stage_min_max_M2', 'stage_avg_mean_M2', 'stage_avg_min_M2',\n",
      "       'stage_avg_max_M2', 'ice_thickness_mean_M2', 'ice_thickness_min_M2',\n",
      "       'ice_thickness_max_M2', 'snow_height_mean_M2', 'snow_height_min_M2',\n",
      "       'snow_height_max_M2', 'discharge_mean_M2', 'discharge_min_M2',\n",
      "       'discharge_max_M2', 'ice_thickness_none_mean_M2',\n",
      "       'snow_height_none_mean_M2', 'data_qual_M2', 'precipitation_observed_M2',\n",
      "       'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2',\n",
      "       'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2',\n",
      "       'sunshine_hours_M2'],\n",
      "      dtype='object')\n",
      "M2\n",
      "(3906, 104)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'ice_thickness_none_mean_M3', 'snow_height_none_mean_M3',\n",
      "       'data_qual_M3', 'precipitation_observed_M3',\n",
      "       'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3',\n",
      "       'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3',\n",
      "       'sunshine_hours_M3', 'M2_M3'],\n",
      "      dtype='object', length=104)\n",
      "(3906, 133)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'snow_height_none_mean_M4', 'data_qual_M4', 'precipitation_observed_M4',\n",
      "       'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4',\n",
      "       'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4',\n",
      "       'sunshine_hours_M4', 'M2_M4', 'M3_M4'],\n",
      "      dtype='object', length=133)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3906 entries, 0 to 3905\n",
      "Columns: 133 entries, year to M3_M4\n",
      "dtypes: float64(126), int64(7)\n",
      "memory usage: 4.0 MB\n"
     ]
    }
   ],
   "source": [
    "month_columns = ['stage_max_mean', 'stage_max_min',\n",
    "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
    "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
    "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
    "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
    "       'discharge_mean', 'discharge_min', 'discharge_max',\n",
    "       'ice_thickness_none_mean', 'snow_height_none_mean',\n",
    "       'data_qual', 'precipitation_observed',\n",
    "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
    "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
    "       'sunshine_hours']\n",
    "\n",
    "\n",
    "print(test.shape)\n",
    "print(DATA.shape)\n",
    "DATA_test = test.merge(DATA, on = ['year','hyd_station_id', 'M1'], suffixes=('','_M1'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "print('M1')\n",
    "\n",
    "\n",
    "# # DATA.rename(columns = {'M1':'M2'}, inplace=True)\n",
    "month_columns +=  ['year','hyd_station_id' , 'M2']\n",
    "# month_columns\n",
    "DATA_test = DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M2'], suffixes=('','_M2'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "print('M2')\n",
    "month_columns +=  ['M3']\n",
    "# DATA.rename(columns = {'M2':'M3'}, inplace=True)\n",
    "DATA_test = DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M3'], suffixes=('','_M3'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "month_columns +=  [ 'M4']\n",
    "# DATA.rename(columns = {'M3':'M4'}, inplace=True)\n",
    "DATA_test= DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M4'], suffixes=('','_M4'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "\n",
    "DATA_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
       "       'M4', 'lat', 'lon',\n",
       "       ...\n",
       "       'snow_height_none_mean_M4', 'data_qual_M4', 'precipitation_observed_M4',\n",
       "       'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4',\n",
       "       'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4',\n",
       "       'sunshine_hours_M4', 'M2_M4', 'M3_M4'],\n",
       "      dtype='object', length=133)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train.drop([\"M2_M1\", \"M3_M4\", \"M3_M1\",\"M2_M3\", \"M4_M1\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_test.drop([\"M2_M1\",\"M2_M4\", \"M3_M4\", \"M3_M1\",\"M2_M3\", \"M4_M1\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in DATA_train.columns:\n",
    "#     print(str('\"'+col+'\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока выбросил  из DATA  'day_1month',  \n",
    "#   \"discharge_mean_M4\", \"discharge_min_M4\",\n",
    "# \"discharge_mean_M4\", \"discharge_min_M4\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = [\"year\", \"hyd_station_id\",\"day_for_pred\", \"M1\",\"M2\",\"M3\",\"M4\",\n",
    "             \"lat\",\"lon\",\"distance_from_source\",\"drainage_area\",\"z_null\",\"month\",\"stage_max_mean\",\n",
    "             \"stage_max_min\",\"stage_max_max\",\"stage_min_mean\",\n",
    "             \"stage_min_min\",\"stage_min_max\",\"stage_avg_mean\",\"stage_avg_min\",\"stage_avg_max\",\n",
    "             \"ice_thickness_mean\",\"ice_thickness_min\",\"ice_thickness_max\",\"snow_height_mean\",\"snow_height_min\",\n",
    "             \"snow_height_max\",\"discharge_mean\",\"discharge_min\",\"discharge_max\",\n",
    "             \"lat_met\",\"lon_met\",\"z\",\"dist\",\"precipitation_observed\",\n",
    "             \"precipitation_corrected\",  \"precipitation_corrected_liquid\", \"precipitation_corrected_mixed\", \n",
    "             \"precipitation_corrected_solid\", \"sunshine_hours\", \"day\", \"stage_max_mean_M2\",\n",
    "            \"stage_max_min_M2\",\"stage_max_max_M2\",\"stage_min_mean_M2\",\"stage_min_min_M2\",\"stage_min_max_M2\",\n",
    "             \"stage_avg_mean_M2\",\"stage_avg_min_M2\",\"stage_avg_max_M2\",\"ice_thickness_mean_M2\",\n",
    "             \"ice_thickness_min_M2\",\"ice_thickness_max_M2\",\"snow_height_mean_M2\",\"snow_height_min_M2\",\n",
    "             \"snow_height_max_M2\",\"discharge_mean_M2\",\"discharge_min_M2\",\"discharge_max_M2\",\n",
    "             \"precipitation_observed_M2\",\"precipitation_corrected_M2\",\n",
    "             \"precipitation_corrected_liquid_M2\",\"precipitation_corrected_mixed_M2\",\"precipitation_corrected_solid_M2\",\n",
    "             \"sunshine_hours_M2\",\"stage_max_mean_M3\",\"stage_max_min_M3\",\"stage_max_max_M3\",\"stage_min_mean_M3\",\n",
    "             \"stage_min_min_M3\",\"stage_min_max_M3\",\"stage_avg_mean_M3\",\"stage_avg_min_M3\",\"stage_avg_max_M3\",\n",
    "             \"ice_thickness_mean_M3\",\"ice_thickness_min_M3\",\"ice_thickness_max_M3\",\"snow_height_mean_M3\",\n",
    "             \"snow_height_min_M3\",\"snow_height_max_M3\",\"discharge_mean_M3\",\"discharge_min_M3\",\"discharge_max_M3\",\n",
    "             \"data_qual_M3\", \"precipitation_observed_M3\", \n",
    "             \"precipitation_corrected_M3\", \"precipitation_corrected_liquid_M3\", \"precipitation_corrected_mixed_M3\",\n",
    "             \"precipitation_corrected_solid_M3\", \"sunshine_hours_M3\", \"stage_max_mean_M4\", \"stage_max_min_M4\",\n",
    "             \"stage_max_max_M4\", \"stage_min_mean_M4\", \"stage_min_min_M4\", \"stage_min_max_M4\", \"stage_avg_mean_M4\",\n",
    "             \"stage_avg_min_M4\", \"stage_avg_max_M4\", \"ice_thickness_mean_M4\", \"ice_thickness_min_M4\",\n",
    "             \"ice_thickness_max_M4\", \"snow_height_mean_M4\", \"snow_height_min_M4\", \"snow_height_max_M4\",\n",
    "             \"discharge_mean_M4\", \"discharge_min_M4\", \"discharge_max_M4\", \n",
    "             \"data_qual_M4\", \"precipitation_observed_M4\", \"precipitation_corrected_M4\",\n",
    "             \"precipitation_corrected_liquid_M4\", \"precipitation_corrected_mixed_M4\", \"precipitation_corrected_solid_M4\",\n",
    "             \"sunshine_hours_M4\",]\n",
    "\n",
    "y_columns = ['ice_jam']\n",
    "\n",
    "# index = ['hyd_station_id','year', 'month', 'day', 'lat_met', 'lon_met',]\n",
    "\n",
    "# const_numerical_features = [ 'lat', 'lon', 'distance_from_source'  'drainage_area','z_null', 'z','dist' ]\n",
    "\n",
    "# Числовые признаки\n",
    "numerical_features = [ \"M1\",\"M2\",\"M3\",\"M4\",\n",
    "             \"lat\",\"lon\",\"distance_from_source\",\"drainage_area\",\"z_null\",\"month\",\"stage_max_mean\",\n",
    "             \"stage_max_min\",\"stage_max_max\",\"stage_min_mean\",\n",
    "             \"stage_min_min\",\"stage_min_max\",\"stage_avg_mean\",\"stage_avg_min\",\"stage_avg_max\",\n",
    "             \"ice_thickness_mean\",\"ice_thickness_min\",\"ice_thickness_max\",\"snow_height_mean\",\"snow_height_min\",\n",
    "             \"snow_height_max\",\"discharge_mean\",\"discharge_min\",\"discharge_max\",\n",
    "             \"lat_met\",\"lon_met\",\"z\",\"dist\",\"precipitation_observed\",\n",
    "             \"precipitation_corrected\",  \"precipitation_corrected_liquid\", \"precipitation_corrected_mixed\", \n",
    "             \"precipitation_corrected_solid\", \"sunshine_hours\", \"day\", \"stage_max_mean_M2\",\n",
    "            \"stage_max_min_M2\",\"stage_max_max_M2\",\"stage_min_mean_M2\",\"stage_min_min_M2\",\"stage_min_max_M2\",\n",
    "             \"stage_avg_mean_M2\",\"stage_avg_min_M2\",\"stage_avg_max_M2\",\"ice_thickness_mean_M2\",\n",
    "             \"ice_thickness_min_M2\",\"ice_thickness_max_M2\",\"snow_height_mean_M2\",\"snow_height_min_M2\",\n",
    "             \"snow_height_max_M2\",\"discharge_mean_M2\",\"discharge_min_M2\",\"discharge_max_M2\",\n",
    "             \"precipitation_observed_M2\",\"precipitation_corrected_M2\",\n",
    "             \"precipitation_corrected_liquid_M2\",\"precipitation_corrected_mixed_M2\",\"precipitation_corrected_solid_M2\",\n",
    "             \"sunshine_hours_M2\",\"stage_max_mean_M3\",\"stage_max_min_M3\",\"stage_max_max_M3\",\"stage_min_mean_M3\",\n",
    "             \"stage_min_min_M3\",\"stage_min_max_M3\",\"stage_avg_mean_M3\",\"stage_avg_min_M3\",\"stage_avg_max_M3\",\n",
    "             \"ice_thickness_mean_M3\",\"ice_thickness_min_M3\",\"ice_thickness_max_M3\",\"snow_height_mean_M3\",\n",
    "             \"snow_height_min_M3\",\"snow_height_max_M3\",\"discharge_mean_M3\",\"discharge_min_M3\",\"discharge_max_M3\",\n",
    "             \"data_qual_M3\", \"precipitation_observed_M3\", \n",
    "             \"precipitation_corrected_M3\", \"precipitation_corrected_liquid_M3\", \"precipitation_corrected_mixed_M3\",\n",
    "             \"precipitation_corrected_solid_M3\", \"sunshine_hours_M3\", \"stage_max_mean_M4\", \"stage_max_min_M4\",\n",
    "             \"stage_max_max_M4\", \"stage_min_mean_M4\", \"stage_min_min_M4\", \"stage_min_max_M4\", \"stage_avg_mean_M4\",\n",
    "             \"stage_avg_min_M4\", \"stage_avg_max_M4\", \"ice_thickness_mean_M4\", \"ice_thickness_min_M4\",\n",
    "             \"ice_thickness_max_M4\", \"snow_height_mean_M4\", \"snow_height_min_M4\", \"snow_height_max_M4\",\n",
    "             \"discharge_mean_M4\", \"discharge_min_M4\", \"discharge_max_M4\",\n",
    "             \"data_qual_M4\", \"precipitation_observed_M4\", \"precipitation_corrected_M4\",\n",
    "             \"precipitation_corrected_liquid_M4\", \"precipitation_corrected_mixed_M4\", \"precipitation_corrected_solid_M4\",\n",
    "             \"sunshine_hours_M4\",]\n",
    "\n",
    "# Категориальные признаки\n",
    "categorical_features = [ \"year\", \"hyd_station_id\",\"day_for_pred\", ]\n",
    "\n",
    "\n",
    "DATA_train.fillna(0,inplace=True)\n",
    "X = DATA_train[X_columns]\n",
    "y = DATA_train['ice_jam']\n",
    "\n",
    "\n",
    "DATA_test.fillna(0,inplace=True)\n",
    "X_test = DATA_test[X_columns]\n",
    "y_test = DATA_test['ice_jam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice_thickness_none_mean\n",
      "snow_height_none_mean\n",
      "data_qual\n",
      "ice_thickness_none_mean_M2\n",
      "snow_height_none_mean_M2\n",
      "data_qual_M2\n",
      "ice_thickness_none_mean_M3\n",
      "snow_height_none_mean_M3\n"
     ]
    }
   ],
   "source": [
    "dro_col = [28, 29, 34, 60, 61, 62, 87, 88]\n",
    "for c in dro_col:\n",
    "    print(X_columns[c+3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_train['ice_thickness_none_mean_M3'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'snow_height_none_mean_M4' 'ice_thickness_none_mean_M4'\n",
    "# ice_thickness_none_mean\n",
    "# snow_height_none_mean\n",
    "# data_qual\n",
    "# ice_thickness_none_mean_M2\n",
    "# snow_height_none_mean_M2\n",
    "# data_qual_M2\n",
    "# ice_thickness_none_mean_M3\n",
    "# snow_height_none_mean_M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11277, 128)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "print(DATA_train.shape)\n",
    "\n",
    "#DATA_train_new = SelectKBest(chi2, k=100).fit_transform(DATA_train.drop('ice_jam', axis=1), DATA_train['ice_jam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"145a401a-14e7-46f2-b4fe-b2a93560da77\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"145a401a-14e7-46f2-b4fe-b2a93560da77\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('data_transformer',\n",
       "                                  ColumnTransformer(transformers=[('numerical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    StandardScaler()),\n",
       "                                                                                   ('filter',\n",
       "                                                                                    SelectKBest(k=100))]),\n",
       "                                                                   ['M1', 'M2',\n",
       "                                                                    'M3', 'M4',\n",
       "                                                                    'lat',\n",
       "                                                                    'lon',\n",
       "                                                                    'distance_from_source',\n",
       "                                                                    'drainage_area',\n",
       "                                                                    'z_null',\n",
       "                                                                    'month',\n",
       "                                                                    'stage_max_mean',\n",
       "                                                                    'stage_max_min',\n",
       "                                                                    'stage_max_max',\n",
       "                                                                    'stage_min_mean',\n",
       "                                                                    'stag...\n",
       "                                                                    'snow_height_min',\n",
       "                                                                    'snow_height_max',\n",
       "                                                                    'discharge_mean',\n",
       "                                                                    'discharge_min',\n",
       "                                                                    'discharge_max',\n",
       "                                                                    'lat_met',\n",
       "                                                                    'lon_met', ...]),\n",
       "                                                                  ('categorical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   ('onehot',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                   ['year',\n",
       "                                                                    'hyd_station_id',\n",
       "                                                                    'day_for_pred'])]))])),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(max_depth=5, n_jobs=-1, random_state=42,\n",
       "                                        verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"976d8537-2088-4464-8df3-7e7ffc8d767e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"976d8537-2088-4464-8df3-7e7ffc8d767e\">preprocessor: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('data_transformer',\n",
       "                 ColumnTransformer(transformers=[('numerical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('filter',\n",
       "                                                                   SelectKBest(k=100))]),\n",
       "                                                  ['M1', 'M2', 'M3', 'M4',\n",
       "                                                   'lat', 'lon',\n",
       "                                                   'distance_from_source',\n",
       "                                                   'drainage_area', 'z_null',\n",
       "                                                   'month', 'stage_max_mean',\n",
       "                                                   'stage_max_min',\n",
       "                                                   'stage_max_max',\n",
       "                                                   'stage_min_mean',\n",
       "                                                   'stage_min_min',\n",
       "                                                   'stage_min_max',\n",
       "                                                   'stag...\n",
       "                                                   'stage_avg_max',\n",
       "                                                   'ice_thickness_mean',\n",
       "                                                   'ice_thickness_min',\n",
       "                                                   'ice_thickness_max',\n",
       "                                                   'snow_height_mean',\n",
       "                                                   'snow_height_min',\n",
       "                                                   'snow_height_max',\n",
       "                                                   'discharge_mean',\n",
       "                                                   'discharge_min',\n",
       "                                                   'discharge_max', 'lat_met',\n",
       "                                                   'lon_met', ...]),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['year', 'hyd_station_id',\n",
       "                                                   'day_for_pred'])]))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"26afdcff-1a53-4158-83e3-0aa6fdcfed1c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"26afdcff-1a53-4158-83e3-0aa6fdcfed1c\">data_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('numerical',\n",
       "                                 Pipeline(steps=[('imputer', StandardScaler()),\n",
       "                                                 ('filter',\n",
       "                                                  SelectKBest(k=100))]),\n",
       "                                 ['M1', 'M2', 'M3', 'M4', 'lat', 'lon',\n",
       "                                  'distance_from_source', 'drainage_area',\n",
       "                                  'z_null', 'month', 'stage_max_mean',\n",
       "                                  'stage_max_min', 'stage_max_max',\n",
       "                                  'stage_min_mean', 'stage_min_min',\n",
       "                                  'stage_min_max', 'stage_avg_mean',\n",
       "                                  'stage_avg_min', 'stage_avg_max',\n",
       "                                  'ice_thickness_mean', 'ice_thickness_min',\n",
       "                                  'ice_thickness_max', 'snow_height_mean',\n",
       "                                  'snow_height_min', 'snow_height_max',\n",
       "                                  'discharge_mean', 'discharge_min',\n",
       "                                  'discharge_max', 'lat_met', 'lon_met', ...]),\n",
       "                                ('categorical',\n",
       "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['year', 'hyd_station_id', 'day_for_pred'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ac95833f-5e9a-42f8-bdc6-11126ecd9356\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ac95833f-5e9a-42f8-bdc6-11126ecd9356\">numerical</label><div class=\"sk-toggleable__content\"><pre>['M1', 'M2', 'M3', 'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null', 'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean', 'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean', 'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min', 'discharge_max', 'lat_met', 'lon_met', 'z', 'dist', 'precipitation_observed', 'precipitation_corrected', 'precipitation_corrected_liquid', 'precipitation_corrected_mixed', 'precipitation_corrected_solid', 'sunshine_hours', 'day', 'stage_max_mean_M2', 'stage_max_min_M2', 'stage_max_max_M2', 'stage_min_mean_M2', 'stage_min_min_M2', 'stage_min_max_M2', 'stage_avg_mean_M2', 'stage_avg_min_M2', 'stage_avg_max_M2', 'ice_thickness_mean_M2', 'ice_thickness_min_M2', 'ice_thickness_max_M2', 'snow_height_mean_M2', 'snow_height_min_M2', 'snow_height_max_M2', 'discharge_mean_M2', 'discharge_min_M2', 'discharge_max_M2', 'precipitation_observed_M2', 'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2', 'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2', 'sunshine_hours_M2', 'stage_max_mean_M3', 'stage_max_min_M3', 'stage_max_max_M3', 'stage_min_mean_M3', 'stage_min_min_M3', 'stage_min_max_M3', 'stage_avg_mean_M3', 'stage_avg_min_M3', 'stage_avg_max_M3', 'ice_thickness_mean_M3', 'ice_thickness_min_M3', 'ice_thickness_max_M3', 'snow_height_mean_M3', 'snow_height_min_M3', 'snow_height_max_M3', 'discharge_mean_M3', 'discharge_min_M3', 'discharge_max_M3', 'data_qual_M3', 'precipitation_observed_M3', 'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3', 'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3', 'sunshine_hours_M3', 'stage_max_mean_M4', 'stage_max_min_M4', 'stage_max_max_M4', 'stage_min_mean_M4', 'stage_min_min_M4', 'stage_min_max_M4', 'stage_avg_mean_M4', 'stage_avg_min_M4', 'stage_avg_max_M4', 'ice_thickness_mean_M4', 'ice_thickness_min_M4', 'ice_thickness_max_M4', 'snow_height_mean_M4', 'snow_height_min_M4', 'snow_height_max_M4', 'discharge_mean_M4', 'discharge_min_M4', 'discharge_max_M4', 'data_qual_M4', 'precipitation_observed_M4', 'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4', 'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4', 'sunshine_hours_M4']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2a12c3de-75bc-4476-a7ae-f551eab32684\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2a12c3de-75bc-4476-a7ae-f551eab32684\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cf868d7f-8dce-4741-b364-8573c201af00\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cf868d7f-8dce-4741-b364-8573c201af00\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=100)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6328e65a-cc1f-429c-af69-cb2b5b84a9e7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6328e65a-cc1f-429c-af69-cb2b5b84a9e7\">categorical</label><div class=\"sk-toggleable__content\"><pre>['year', 'hyd_station_id', 'day_for_pred']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"91f8a822-5cba-4ddf-ae37-0d5a86690294\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"91f8a822-5cba-4ddf-ae37-0d5a86690294\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fddf55ff-ba41-44b7-90f4-42ed60798d64\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fddf55ff-ba41-44b7-90f4-42ed60798d64\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2d97fae6-17a0-4f38-8fc5-8e6cda074422\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2d97fae6-17a0-4f38-8fc5-8e6cda074422\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, n_jobs=-1, random_state=42, verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('data_transformer',\n",
       "                                  ColumnTransformer(transformers=[('numerical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    StandardScaler()),\n",
       "                                                                                   ('filter',\n",
       "                                                                                    SelectKBest(k=100))]),\n",
       "                                                                   ['M1', 'M2',\n",
       "                                                                    'M3', 'M4',\n",
       "                                                                    'lat',\n",
       "                                                                    'lon',\n",
       "                                                                    'distance_from_source',\n",
       "                                                                    'drainage_area',\n",
       "                                                                    'z_null',\n",
       "                                                                    'month',\n",
       "                                                                    'stage_max_mean',\n",
       "                                                                    'stage_max_min',\n",
       "                                                                    'stage_max_max',\n",
       "                                                                    'stage_min_mean',\n",
       "                                                                    'stag...\n",
       "                                                                    'snow_height_min',\n",
       "                                                                    'snow_height_max',\n",
       "                                                                    'discharge_mean',\n",
       "                                                                    'discharge_min',\n",
       "                                                                    'discharge_max',\n",
       "                                                                    'lat_met',\n",
       "                                                                    'lon_met', ...]),\n",
       "                                                                  ('categorical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   ('onehot',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                   ['year',\n",
       "                                                                    'hyd_station_id',\n",
       "                                                                    'day_for_pred'])]))])),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(max_depth=5, n_jobs=-1, random_state=42,\n",
       "                                        verbose=True))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем SimpleImputer и будем искать различные скейлеры с помощью GridSearchCV\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", StandardScaler()), (\"filter\", SelectKBest(f_classif, k=100))]  # StandardScaler()\n",
    ")\n",
    "\n",
    "# Применение SimpleImputer, а затем OneHotEncoder\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Собираем воедино трансформеры для числовых и категориальных признаков\n",
    "data_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", numerical_transformer, numerical_features),\n",
    "        (\"categorical\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создание конвейера препроцессора, который сначала преобразует данные и затем применяет PCA.\n",
    "preprocessor = Pipeline(steps=[(\"data_transformer\", data_transformer)])\n",
    "#  ('reduce_dim', PCA())])\n",
    "\n",
    "# we are using LinearRegression here\n",
    "classifier = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        #(\"classifier\", LinearRegression(copy_X=True, fit_intercept=True, normalize=True, n_jobs=-1),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42, #min_samples_split=4, #min_samples_leaf = 10,\n",
    "                                             max_depth=5, n_estimators=100, verbose=True,  n_jobs=-1)\n",
    "           \n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели:  {'classifier__max_depth': 6, 'classifier__min_samples_leaf': 5, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 90, 'preprocessor__data_transformer__numerical__filter__k': 110}\n",
      "\n",
      "\n",
      "Метрика f1:  0.009504950495049506\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# classifier = Pipeline(\n",
    "#     steps=[\n",
    "#     (\"preprocessor\", preprocessor),\n",
    "#     (\"classifier\", RandomForestRegressor(n_jobs=-1, min_samples_split=3, max_depth=4, n_estimators=200, verbose=True))])\n",
    "\n",
    "model = classifier  \n",
    "parameters = {\n",
    "    'classifier__min_samples_leaf':[5],\n",
    "    'classifier__max_depth':[6, 5],\n",
    "    'classifier__n_estimators':[90, 100], # 195\n",
    "    'classifier__min_samples_split':[2, 3],\n",
    "    'preprocessor__data_transformer__numerical__filter__k':[110,113],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(model, parameters, cv=5, n_jobs=-1, scoring= 'f1')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Лучшие параметры модели: \", grid.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Метрика f1: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.016872637393890774\n",
    "# 0.006504065040650407\n",
    "# Метрика f1:  0.009329446064139942 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the input must be equal to or greater than that of the fitted transformer. Transformer n_features is 124 and input n_features is 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-d0b4f83d73c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \"\"\"\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_estimator_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             raise ValueError('Number of features of the input must be equal '\n\u001b[0m\u001b[0;32m    582\u001b[0m                              \u001b[1;34m'to or greater than that of the fitted '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                              \u001b[1;34m'transformer. Transformer n_features is {0} '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the input must be equal to or greater than that of the fitted transformer. Transformer n_features is 124 and input n_features is 2."
     ]
    }
   ],
   "source": [
    "preds = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_col = ['year', 'station_id', 'day', 'ice_jam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-5638d68e5876>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_task_1_random.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ice_jam'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'first_sub.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('test_task_1_random.csv') \n",
    "submission['ice_jam'] = preds\n",
    "submission.to_csv('first_sub.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим годы на train и test сбалансированно по суммарной продолжительности заторных событий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    26.000000\n",
       "mean      5.307692\n",
       "std       3.792300\n",
       "min       0.000000\n",
       "25%       3.000000\n",
       "50%       5.000000\n",
       "75%       8.000000\n",
       "max      16.000000\n",
       "Name: ice_jam, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Суммарное количество заторов в году\n",
    "jams_by_year = DATA_train.groupby('year').sum()['ice_jam'].to_frame().reset_index()\n",
    "jams_by_year['ice_jam'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 3]    9\n",
       "(3, 5]     7\n",
       "(5, 8]     6\n",
       "(8, 16]    4\n",
       "Name: ice_jam_bins, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разделим на бины по квартилям\n",
    "bins = [-1, 3, 5, 8, 16]\n",
    "jams_by_year['ice_jam_bins'] = pd.cut(jams_by_year['ice_jam'], bins)\n",
    "X_length = jams_by_year[['year', 'ice_jam']]\n",
    "y_length = jams_by_year['ice_jam_bins']\n",
    "y_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим годы на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_length, \n",
    "                                                    y_length, \n",
    "                                                    test_size=0.3,  \n",
    "                                                    stratify=y_length, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи из гидроданных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Мы не можем использовать данные из будущего: всё, что происходит после заторного периода, относится уже к следующему году.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Внесем не темпоральные данные\n",
    "# # DATA = pd.merge(DATA, hydro_coord[['hyd_station_id', \n",
    "# #                                 'distance_from_source', \n",
    "# #                                 'drainage_area', \n",
    "# #                                 'z_null']], on='hyd_station_id', how='left')\n",
    "\n",
    "# # Возьмем также данные из ежедневных наблюдений\n",
    "# hydro_1day = pd.read_csv(data_dir + 'hydro_1day.csv',\n",
    "#                    parse_dates=['date'])\n",
    "# hydro_1day.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydro_1day['station_id'] = hydro_1day.station_id.astype(int)\n",
    "# meteo_1day['station_id'] = meteo_1day['station_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meteo_1day.head()\n",
    "# hydro_1day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydro_1day = hydro_1day.merge(meteo_1day, on=['station_id', 'year', 'month', 'day'], how='inner')\n",
    "# hydro_1day = hydro_1day.fillna(0)\n",
    "# hydro_1day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скорректируем год, в который доступно наблюдение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Определим для наблюдения год, в который оно доступно\n",
    "\n",
    "# # Маска-окно между заторным периодом и концом года\n",
    "# def after_jam_window(row, local=False):\n",
    "#     if local:\n",
    "#         month = row.month_local\n",
    "#         day = row.date_local.day\n",
    "#     else:\n",
    "#         month = row.month\n",
    "#         day = row.date.day\n",
    "#     return (((month == 6) and (day > 3))\n",
    "#             or (month in [7, 8, 9, 10, 11, 12]))\n",
    "\n",
    "# # Год относительно бизнес-логики\n",
    "# def target_year(row, local=False):\n",
    "#     if local:\n",
    "#         year = row.year_local\n",
    "#     else:\n",
    "#         year = row.year\n",
    "#     if after_jam_window(row):\n",
    "#         return year + 1\n",
    "#     else:\n",
    "#         return year\n",
    "    \n",
    "# # hydro_1day['target_year'] = hydro_1day.apply(target_year, axis=1)\n",
    "\n",
    "# # # Календарный год и день больше не нужны\n",
    "# # hydro_1day.drop(columns=['year', 'date', 'day'], axis=1, inplace=True)\n",
    "\n",
    "# DATA['target_year'] = hydro_1day.apply(target_year, axis=1)\n",
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'station_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-fd00a6030406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'station_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target_year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhydro_1day_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhydro_1day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mhydro_1day_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhydro_1day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mhydro_1day_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhydro_1day\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   6509\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6511\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   6512\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6513\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    526\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    779\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'station_id'"
     ]
    }
   ],
   "source": [
    "# Гидро – сделаем ресэмплинг по месяцу\n",
    "\n",
    "index = ['station_id', 'month', 'target_year']\n",
    "\n",
    "hydro_1day_mean = hydro_1day.groupby(index).mean().add_prefix('mean_').reset_index()\n",
    "hydro_1day_max = hydro_1day.groupby(index).max().add_prefix('max_').reset_index()\n",
    "hydro_1day_min = hydro_1day.groupby(index).min().add_prefix('min_').reset_index()\n",
    "hydro_1day_std = hydro_1day.groupby(index).std().add_prefix('std_').reset_index()\n",
    "data_frames = [hydro_1day_mean, hydro_1day_max, hydro_1day_min, hydro_1day_std]\n",
    "\n",
    "hydro_monthly = pd.concat(data_frames, axis=1)\n",
    "hydro_monthly = hydro_monthly.loc[:,~hydro_monthly.columns.duplicated()]\n",
    "hydro_monthly.sort_values(index).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    station, target_year = df.name\n",
    "    result = pd.DataFrame()\n",
    "    for month, mdf in df.groupby('month'):\n",
    "        m_feats = mdf[df.columns[4:]].add_prefix(str(month) + '_').reset_index(drop=True)\n",
    "        result = pd.concat([result, m_feats], axis=1)\n",
    "    return result.reset_index(drop=True)\n",
    "        \n",
    "hydro_features = hydro_monthly.groupby(['station_id', 'target_year']).apply(make_features)\n",
    "hydro_features = hydro_features.reset_index(level=2, drop=True).reset_index()\n",
    "hydro_features.dropna(how='all', axis=1, inplace=True)\n",
    "hydro_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи в основной датасет\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Важно: merge делаем по target_year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.merge(DATA, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = DATA.columns.to_list()\n",
    "DATA = DATA[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]\n",
    "DATA.dropna(how='any',inplace=True)\n",
    "DATA.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ids, data, target = DATA[DATA.columns[:4]], DATA[DATA.columns[4:-1]], DATA[DATA.columns[-1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "transformed_data = scaler.transform(data)\n",
    "norm_df = pd.concat([ids, pd.DataFrame(transformed_data, columns = DATA.columns[4:-1]), target], axis=1)\n",
    "norm_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на трейн и тест исходя из target_year\n",
    "\n",
    "test = norm_df[~norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "train = norm_df[norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "\n",
    "# target_year больше не нужна\n",
    "\n",
    "test.drop(columns=['target_year'], inplace=True)\n",
    "train.drop(columns=['target_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поделим данные на предикторы и таргет\n",
    "\n",
    "X_train, y_train = train.iloc[:, :-1], train.ice_jam\n",
    "X_test, y_test = test.iloc[:, :-1], test.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  discharge_mean_M4 дропаем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальное разбиение фичей на группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Удаляем дубли\n",
    "\n",
    "# clear_DATA = DATA[X_columns+y_columns].copy()\n",
    "\n",
    "# print('До',clear_DATA.shape)\n",
    "# clear_DATA.drop_duplicates(inplace=True)\n",
    "# print('После', clear_DATA.shape)\n",
    "\n",
    "\n",
    "# X = clear_DATA[X_columns]\n",
    "# y = clear_DATA['MarkPercent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение классификатора и подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Pipeline(\n",
    "#     steps=[\n",
    "#     (\"preprocessor\", preprocessor),\n",
    "#     (\"classifier\", RandomForestRegressor(n_jobs=-1, min_samples_split=3, max_depth=4, n_estimators=200, verbose=True))])\n",
    "\n",
    "model = classifier  \n",
    "parameters = {\n",
    "    'classifier__min_samples_leaf':[11],\n",
    "    'classifier__max_depth':[7],\n",
    "    'classifier__n_estimators':[155], # 195\n",
    "    'classifier__min_samples_split':[4]\n",
    "\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=5, n_jobs=-1, scoring= 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Лучшие параметры модели: \", grid.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Метрика RMSE: \", abs(grid.best_score_)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "full_importance = model.named_steps['classifier'].feature_importances_  # # get importance\n",
    "\n",
    "full_importance, X_cols = zip(*sorted(zip(full_importance, X.columns)))\n",
    "\n",
    "pyplot.barh([x for x in X_cols[:15]], full_importance[:15])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим простой классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lsvc = SVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "selector = RFE(LogisticRegression(max_iter=5000),\n",
    "               n_features_to_select=200, step=10).fit(X_train, y_train)\n",
    "X_reduced = selector.transform(X_train)\n",
    "X_reduced_test = selector.transform(X_test)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "parameters = {'pca__n_components': list(range(400,500,50)), 'svc__kernel':('linear', 'rbf'), 'svc__C':[1,10]}\n",
    "clf = Pipeline([('pca', pca), ('svc', SVC(class_weight='balanced'))])\n",
    "GS = GridSearchCV(clf, parameters, scoring='f1_macro')\n",
    "GS.fit(X_train, y_train)\n",
    "print(GS.cv_results_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = GS.predict(X_test)\n",
    "recall_score(y_test, prediction), \\\n",
    "    precision_score(y_test, prediction), f1_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "#test_real = pd.read_csv(data_dir + '../test_SECRET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_real, hydro_coord[['station_id', \n",
    "                                'distance_from_source', \n",
    "                                'drainage_area', \n",
    "                                'z_null']], on='station_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'year' in hydro_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_X, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = test_X.columns.to_list()\n",
    "test_X = test_X[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, data, target = test_X[test_X.columns[:4]], test_X[test_X.columns[4:-1]], test_X[test_X.columns[-1]]\n",
    "\n",
    "transformed_data = scaler.transform(data)\n",
    "test_X = pd.concat([ids, pd.DataFrame(transformed_data, columns = DATA.columns[4:-1]), target], axis=1)\n",
    "X_test_real, y_test_real = test_X.iloc[:, :-1], test_X.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real.drop('target_year',inplace=True,axis=1)\n",
    "X_test_real = X_test_real.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_real_reduced = selector.transform(X_test_real)\n",
    "prediction =  GS.predict(X_test_real)\n",
    "accuracy_score(y_test_real, prediction), recall_score(y_test_real, prediction), \\\n",
    "    precision_score(y_test_real, prediction), f1_score(y_test_real, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

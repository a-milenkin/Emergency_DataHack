{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import os\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import (RepeatedKFold, cross_val_score,\n",
    "                              train_test_split, GridSearchCV,GridSearchCV)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam\n",
       "0  2000        3019    1      0.0\n",
       "1  2000        3019    2      0.0\n",
       "2  2000        3019    3      0.0\n",
       "3  2000        3019    4      0.0\n",
       "4  2000        3019    5      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "train = pd.read_csv('track_1/train.csv') # main_df\n",
    "print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# города\n",
    "# Киренск, посёлок городского типа Витим, посёлок городского типа Пеледуй, село Крестовский \n",
    "# Лесоуùасток, город Ленск, город Олёкминск, город Покровск, город Якутск, село Батамай, \n",
    "# посёлок городского типа Сангар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"Images/post_map.PNG\"  width=900></p>\n",
    "<h3 style=\"text-align: center;\"><b>Карта метеостанций и гидропостов</b></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузим всё\n",
    "# train = pd.read_csv('1_track_extra_train/hydro_1day.csv') # main_df\n",
    "# print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "# train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузим всё\n",
    "# train = pd.read_csv('track_1/hydro_1day.csv') # main_df\n",
    "# print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "# train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hydro_1day.csv', 'meteo_1day.csv', 'meteo_1month.csv', 'meteo_3hours.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к директории с данными\n",
    "\n",
    "data_dir_new = '1_track_extra_train/'\n",
    "os.listdir(data_dir_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hydro_1day.csv',\n",
       " 'hydro_coord.csv',\n",
       " 'ice_saw.csv',\n",
       " 'meteo_1day.csv',\n",
       " 'meteo_1month.csv',\n",
       " 'meteo_3hours.csv',\n",
       " 'meteo_coord.csv',\n",
       " 'reference_horiz_visib.csv',\n",
       " 'reference_water_codes.csv',\n",
       " 'test.csv',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к директории с данными\n",
    "\n",
    "data_dir = './track_1/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam\n",
       "0  2000        3019    1      0.0\n",
       "1  2000        3019    2      0.0\n",
       "2  2000        3019    3      0.0\n",
       "3  2000        3019    4      0.0\n",
       "4  2000        3019    5      0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "train = pd.read_csv(data_dir + 'train.csv') # main_df\n",
    "print(train.day.unique())  #Дни больше чем 30 !!!!\n",
    "train.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['year', 'station_id', 'day', 'ice_jam'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим всё\n",
    "test = pd.read_csv(data_dir + 'test.csv') # main_df\n",
    "print(test.day.unique())  #Дни больше чем 30 !!!!\n",
    "test.head()   #Отсчет дней с 21 апреля по 3 июня происходит затор льда. \n",
    "\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подтянем ближайшую к гидростанции метеостанцию\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "import re\n",
    "\n",
    "def merge_coord(df):\n",
    "    df['lat_long'] = df[['lat', 'lon']].apply(tuple, axis=1)\n",
    "    return df\n",
    "\n",
    "def stat_km(point, stat_list):\n",
    "    stations_list=stat_list\n",
    "    lst=[]\n",
    "    if pd.isnull(point):\n",
    "        lst.append(np.nan)\n",
    "    else:\n",
    "        for i in stations_list['lat_long']:\n",
    "            x=geodesic(point, i).km\n",
    "            lst.append(x)\n",
    "            stations_list['dist']=pd.DataFrame(lst)\n",
    "        y=stations_list['station_id'][stations_list['dist'] == stations_list['dist'].min()]\n",
    "        y=y.to_string()\n",
    "        y=re.sub(\"^[0-9]+\", \"\", y)\n",
    "        y=re.sub(\" +\", \"\", y)\n",
    "        return int(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(346130, 48)\n",
      "(10926, 17)\n",
      "(10926, 17)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10926 entries, 0 to 10925\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   met_station_id                  10926 non-null  int64  \n",
      " 1   lat                             10926 non-null  float64\n",
      " 2   lon                             10926 non-null  float64\n",
      " 3   z                               10926 non-null  float64\n",
      " 4   lat_long                        10926 non-null  object \n",
      " 5   dist                            10926 non-null  float64\n",
      " 6   year                            10926 non-null  int64  \n",
      " 7   month                           10926 non-null  int64  \n",
      " 8   data_qual                       8801 non-null   float64\n",
      " 9   precipitation_observed          8801 non-null   float64\n",
      " 10  precipitation_corrected         8801 non-null   float64\n",
      " 11  precipitation_corrected_liquid  8801 non-null   float64\n",
      " 12  precipitation_corrected_mixed   8801 non-null   float64\n",
      " 13  precipitation_corrected_solid   8801 non-null   float64\n",
      " 14  sunshine_hours                  7784 non-null   float64\n",
      " 15  date                            10926 non-null  object \n",
      " 16  day                             10926 non-null  int64  \n",
      "dtypes: float64(11), int64(4), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo_coord = pd.read_csv(data_dir + 'meteo_coord.csv')  # mc\n",
    "meteo_coord.drop(['name'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Гидро\n",
    "hydro_coord = pd.read_csv(data_dir + 'hydro_coord.csv') # hc\n",
    "hydro_coord.drop(columns = ['name'], inplace=True)\n",
    "\n",
    "\n",
    "hydro_coord = merge_coord(hydro_coord)\n",
    "meteo_coord = merge_coord(meteo_coord)\n",
    "hydro_coord['met_station_id'] = meteo_coord.lat_long.apply(lambda x: stat_km(x, meteo_coord))\n",
    "\n",
    "\n",
    "hydro_coord.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "meteo_coord.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "\n",
    "\n",
    "meteo_1day = pd.read_csv(data_dir_new + 'meteo_1day.csv')  #mld\n",
    "meteo_1day.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "# meteo_1day.drop(['date'], inplace=True, axis=1)\n",
    "print(meteo_1day.shape)\n",
    "# print(meteo_1day.columns)\n",
    "#meteo_1day = meteo_coord.merge(meteo_1day, on=['met_station_id'], how='left')\n",
    "\n",
    "meteo_1month = pd.read_csv(data_dir_new + 'meteo_1month.csv')  #mlm\n",
    "meteo_1month.rename(columns = {\"station_id\":'met_station_id'}, inplace=True)\n",
    "meteo_1month = meteo_coord.merge(meteo_1month, on=['met_station_id'], how='left')\n",
    "#meteo_1month.drop(['date'], inplace=True, axis=1)\n",
    "# print(meteo_1month.shape)\n",
    "# print(meteo_1month.columns)\n",
    "# meteo_1month.head(2)\n",
    "\n",
    "\n",
    "print(meteo_1month.shape)\n",
    "# meteo_all = meteo_1day.merge(meteo_1month, on=['met_station_id', 'year', 'month'], \n",
    "#                 how='left')\n",
    "# meteo_all.rename(columns = {\"date_x\":\"date_1day\", \"date_y\":\"date_1month\", \"day_x\":\"day\",\"day_y\":\"day_1month\"}, inplace=True)\n",
    "\n",
    "meteo_all = meteo_1month.copy()\n",
    "#TODOs исправить\n",
    "\n",
    "\n",
    "# meteo_all.fillna(0, inpalce=True) # Закодировать наличие пропуска в толшине воды/льда\n",
    "print(meteo_all.shape)\n",
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOs добавить инфу из meteo_3hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer_horiz_visib = pd.read_csv(data_dir + 'reference_horiz_visib.csv') #rhv\n",
    "# refer_horiz_visib.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гидро"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydro_coord (27, 8)\n",
      "hydro_1day размер (234138, 16)\n",
      "Index(['year', 'hyd_station_id', 'month', 'day', 'date', 'stage_avg',\n",
      "       'stage_min', 'stage_max', 'temp', 'water_code', 'ice_thickness',\n",
      "       'snow_height', 'place', 'discharge', 'ice_thickness_is_none',\n",
      "       'snow_height_is_none'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# # Гидро\n",
    "# hydro_coord = pd.read_csv(data_dir + 'hydro_coord.csv') # hc\n",
    "# hydro_coord.drop(columns = ['name'], inplace=True)\n",
    "# hydro_coord.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "\n",
    "print('hydro_coord', hydro_coord.shape)\n",
    "#print(hydro_coord.columns)\n",
    "# hydro_coord.head(3)\n",
    "\n",
    "\n",
    "hydro_1day = pd.read_csv(data_dir_new + 'hydro_1day.csv',  parse_dates=['date'])  # hld             \n",
    "hydro_1day.rename(columns = {\"station_id\":'hyd_station_id'}, inplace=True)\n",
    "#hydro_1day.drop('date', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def check_is_none(x):  #Заметим что фичу не надо категоризировать!\n",
    "    if x is None: return 1\n",
    "    else: return 0\n",
    "\n",
    "    \n",
    "    \n",
    "hydro_1day['ice_thickness_is_none'] =hydro_1day['ice_thickness'].apply(check_is_none)\n",
    "hydro_1day['ice_thickness'].fillna(hydro_1day['ice_thickness'].mean(), inplace=True)\n",
    "\n",
    "hydro_1day['snow_height_is_none'] =hydro_1day['snow_height'].apply(check_is_none)\n",
    "hydro_1day['snow_height'].fillna(hydro_1day['snow_height'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "print('hydro_1day размер',hydro_1day.shape)\n",
    "print(hydro_1day.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_1day = hydro_1day.groupby(['hyd_station_id', 'year', 'month'], as_index=False).agg(stage_max_mean = ('stage_max', 'mean'),\n",
    "                                                                           stage_max_min = ('stage_max', 'min'),\n",
    "                                                                           stage_max_max = ('stage_max', 'max'),\n",
    "                                                                           \n",
    "                                                                           stage_min_mean = ('stage_min', 'mean'),\n",
    "                                                                           stage_min_min = ('stage_min', 'min'),\n",
    "                                                                           stage_min_max = ('stage_min', 'max'),\n",
    "                                                                           \n",
    "                                                                           stage_avg_mean =('stage_avg', 'mean'),\n",
    "                                                                           stage_avg_min = ('stage_avg', 'min'),\n",
    "                                                                           stage_avg_max = ('stage_avg', 'max'),\n",
    "                                                                        \n",
    "                                                                           ice_thickness_mean = ('ice_thickness', 'mean'),\n",
    "                                                                           ice_thickness_min = ('ice_thickness', 'min'),\n",
    "                                                                           ice_thickness_max = ('ice_thickness', 'max'),\n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                           snow_height_mean = ('snow_height', 'mean'),\n",
    "                                                                           snow_height_min = ('snow_height', 'min'),\n",
    "                                                                           snow_height_max = ('snow_height', 'max'),\n",
    "                                                                           \n",
    "                                                                            \n",
    "                                                                           discharge_mean = ('discharge', 'mean'),\n",
    "                                                                           discharge_min = ('discharge', 'min'),\n",
    "                                                                           discharge_max = ('discharge', 'max'),\n",
    "                                                                                         \n",
    "                                                                           temp_mean = ('temp', 'mean'),\n",
    "                                                                           temp_min = ('temp', 'min'),\n",
    "                                                                           temp_max = ('temp', 'max'),\n",
    "                                                                                         \n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                            \n",
    "                                                                          ice_thickness_none_mean = ('ice_thickness_is_none', 'mean'),\n",
    "                                                                          snow_height_none_mean = ('snow_height_is_none', 'mean')\n",
    "                                                                                        )          \n",
    "                                                                                        \n",
    "                                                                            #water_code\n",
    "                                                                                         \n",
    "                                                                \n",
    "\n",
    "# pd.Series.mode\n",
    "#.agg(lambda x:x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydro_all размер (7869, 33) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hyd_station_id           7869 non-null   int64  \n",
      " 1   lat                      7869 non-null   float64\n",
      " 2   lon                      7869 non-null   float64\n",
      " 3   distance_from_source     7869 non-null   float64\n",
      " 4   drainage_area            7869 non-null   int64  \n",
      " 5   z_null                   7869 non-null   float64\n",
      " 6   lat_long                 7869 non-null   object \n",
      " 7   met_station_id           7869 non-null   int64  \n",
      " 8   year                     7869 non-null   int64  \n",
      " 9   month                    7869 non-null   int64  \n",
      " 10  stage_max_mean           7865 non-null   float64\n",
      " 11  stage_max_min            7865 non-null   float64\n",
      " 12  stage_max_max            7865 non-null   float64\n",
      " 13  stage_min_mean           7865 non-null   float64\n",
      " 14  stage_min_min            7865 non-null   float64\n",
      " 15  stage_min_max            7865 non-null   float64\n",
      " 16  stage_avg_mean           7865 non-null   float64\n",
      " 17  stage_avg_min            7865 non-null   float64\n",
      " 18  stage_avg_max            7865 non-null   float64\n",
      " 19  ice_thickness_mean       7869 non-null   float64\n",
      " 20  ice_thickness_min        7869 non-null   float64\n",
      " 21  ice_thickness_max        7869 non-null   float64\n",
      " 22  snow_height_mean         7869 non-null   float64\n",
      " 23  snow_height_min          7869 non-null   float64\n",
      " 24  snow_height_max          7869 non-null   float64\n",
      " 25  discharge_mean           2991 non-null   float64\n",
      " 26  discharge_min            2991 non-null   float64\n",
      " 27  discharge_max            2991 non-null   float64\n",
      " 28  temp_mean                4600 non-null   float64\n",
      " 29  temp_min                 4600 non-null   float64\n",
      " 30  temp_max                 4600 non-null   float64\n",
      " 31  ice_thickness_none_mean  7869 non-null   int64  \n",
      " 32  snow_height_none_mean    7869 non-null   int64  \n",
      "dtypes: float64(25), int64(7), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "hydro_all = hydro_coord.merge(hydro_1day, on=['hyd_station_id'], how='left')\n",
    "\n",
    "print('hydro_all размер',hydro_all.shape,'\\n')\n",
    "\n",
    "# hydro_all.head(3)\n",
    "#TODOs исправить \n",
    "\n",
    "hydro_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice_saw (1547, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>place</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-04-02</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-03</td>\n",
       "      <td>Ленск</td>\n",
       "      <td>Ослабление прочности льда на р.Нюя в Ленском р...</td>\n",
       "      <td>61.378333</td>\n",
       "      <td>114.565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  place                                               name  \\\n",
       "0  2011-04-01  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "1  2011-04-02  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "2  2011-04-03  Ленск  Ослабление прочности льда на р.Нюя в Ленском р...   \n",
       "\n",
       "         lat      lon  \n",
       "0  61.378333  114.565  \n",
       "1  61.378333  114.565  \n",
       "2  61.378333  114.565  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Анастасия \n",
    "#\n",
    "ice_saw = pd.read_csv(data_dir + 'ice_saw.csv') #rhv\n",
    "#мб фича в том есть ли пробел между точкой и словом\n",
    "print('ice_saw',ice_saw.shape)\n",
    "#Почистим немного данные,чтобы привести к одному виду\n",
    "ice_saw.name = ice_saw.name.str.strip()\n",
    "ice_saw.name = ice_saw.name.str.replace('р. ', 'р.')\n",
    "ice_saw.name = ice_saw.name.str.replace('г. ', 'г.')\n",
    "ice_saw.name = ice_saw.name.str.replace('е Р', 'е, Р')\n",
    "ice_saw.name = ice_saw.name.str.replace('ики Са', 'ика Са')\n",
    "ice_saw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гео is comning\n",
    "#TODO какой гидроцентр по счету сверху \n",
    "#TODO вытащить фичи из названия участка реки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подтянем ближайшую к гидростанции метеостанцию\n",
    "\n",
    "# from geopy.distance import geodesic\n",
    "# import re\n",
    "\n",
    "# def merge_coord(df):\n",
    "#     df['lat_long'] = df[['lat', 'lon']].apply(tuple, axis=1)\n",
    "#     return df\n",
    "\n",
    "# def stat_km(point, stat_list):\n",
    "#     stations_list=stat_list\n",
    "#     lst=[]\n",
    "#     if pd.isnull(point):\n",
    "#         lst.append(np.nan)\n",
    "#     else:\n",
    "#         for i in stations_list['lat_long']:\n",
    "#             x=geodesic(point, i).km\n",
    "#             lst.append(x)\n",
    "#             stations_list['dist']=pd.DataFrame(lst)\n",
    "#         y=stations_list['met_station_id'][stations_list['dist'] == stations_list['dist'].min()]\n",
    "#         y=y.to_string()\n",
    "#         y=re.sub(\"^[0-9]+\", \"\", y)\n",
    "#         y=re.sub(\" +\", \"\", y)\n",
    "#         return int(y)\n",
    "\n",
    "\n",
    "# hydro_coord = merge_coord(hydro_coord)\n",
    "# meteo_coord = merge_coord(meteo_coord)\n",
    "# hydro_coord['closest_hydro'] = meteo_coord.lat_long.apply(lambda x: stat_km(x, meteo_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyd_station_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_source</th>\n",
       "      <th>drainage_area</th>\n",
       "      <th>z_null</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>met_station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>stage_max_mean</th>\n",
       "      <th>stage_max_min</th>\n",
       "      <th>stage_max_max</th>\n",
       "      <th>stage_min_mean</th>\n",
       "      <th>stage_min_min</th>\n",
       "      <th>stage_min_max</th>\n",
       "      <th>stage_avg_mean</th>\n",
       "      <th>stage_avg_min</th>\n",
       "      <th>stage_avg_max</th>\n",
       "      <th>ice_thickness_mean</th>\n",
       "      <th>ice_thickness_min</th>\n",
       "      <th>ice_thickness_max</th>\n",
       "      <th>snow_height_mean</th>\n",
       "      <th>snow_height_min</th>\n",
       "      <th>snow_height_max</th>\n",
       "      <th>discharge_mean</th>\n",
       "      <th>discharge_min</th>\n",
       "      <th>discharge_max</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>ice_thickness_none_mean</th>\n",
       "      <th>snow_height_none_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>-25.580645</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-25.709677</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-25.709677</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>71.622206</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>25.176103</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>2</td>\n",
       "      <td>-25.928571</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-26.107143</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-26.107143</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>73.083349</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>26.563003</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>3</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-27.516129</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>75.233645</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>23.714655</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-8.233333</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3019</td>\n",
       "      <td>57.77</td>\n",
       "      <td>108.07</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>92200</td>\n",
       "      <td>249.38</td>\n",
       "      <td>(57.77, 108.07)</td>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>5</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>200.516129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>75.651536</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>22.898368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.509677</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hyd_station_id    lat     lon  distance_from_source  drainage_area  z_null  \\\n",
       "0            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "1            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "2            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "3            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "4            3019  57.77  108.07                1140.0          92200  249.38   \n",
       "\n",
       "          lat_long  met_station_id  year  month  stage_max_mean  \\\n",
       "0  (57.77, 108.07)           24538  1985      1      -25.580645   \n",
       "1  (57.77, 108.07)           24538  1985      2      -25.928571   \n",
       "2  (57.77, 108.07)           24538  1985      3      -27.516129   \n",
       "3  (57.77, 108.07)           24538  1985      4       -8.233333   \n",
       "4  (57.77, 108.07)           24538  1985      5      200.516129   \n",
       "\n",
       "   stage_max_min  stage_max_max  stage_min_mean  stage_min_min  stage_min_max  \\\n",
       "0          -28.0          -23.0      -25.709677          -28.0          -23.0   \n",
       "1          -28.0          -23.0      -26.107143          -28.0          -23.0   \n",
       "2          -32.0          -19.0      -27.516129          -32.0          -19.0   \n",
       "3          -20.0           15.0       -8.233333          -20.0           15.0   \n",
       "4            0.0          420.0      200.516129            0.0          420.0   \n",
       "\n",
       "   stage_avg_mean  stage_avg_min  stage_avg_max  ice_thickness_mean  \\\n",
       "0      -25.709677          -28.0          -23.0           71.622206   \n",
       "1      -26.107143          -28.0          -23.0           73.083349   \n",
       "2      -27.516129          -32.0          -19.0           75.233645   \n",
       "3       -8.233333          -20.0           15.0           75.651536   \n",
       "4      200.516129            0.0          420.0           75.651536   \n",
       "\n",
       "   ice_thickness_min  ice_thickness_max  snow_height_mean  snow_height_min  \\\n",
       "0          50.000000          75.651536         25.176103        22.898368   \n",
       "1          61.000000          75.651536         26.563003        22.898368   \n",
       "2          69.000000          75.651536         23.714655        22.898368   \n",
       "3          75.651536          75.651536         22.898368        22.898368   \n",
       "4          75.651536          75.651536         22.898368        22.898368   \n",
       "\n",
       "   snow_height_max  discharge_mean  discharge_min  discharge_max  temp_mean  \\\n",
       "0        44.000000             NaN            NaN            NaN        NaN   \n",
       "1        43.000000             NaN            NaN            NaN        NaN   \n",
       "2        35.000000             NaN            NaN            NaN        NaN   \n",
       "3        22.898368             NaN            NaN            NaN   0.050000   \n",
       "4        22.898368             NaN            NaN            NaN   2.509677   \n",
       "\n",
       "   temp_min  temp_max  ice_thickness_none_mean  snow_height_none_mean  \n",
       "0       NaN       NaN                        0                      0  \n",
       "1       NaN       NaN                        0                      0  \n",
       "2       NaN       NaN                        0                      0  \n",
       "3       0.0       0.1                        0                      0  \n",
       "4       0.1       7.4                        0                      0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydro_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hyd_station_id           7869 non-null   int64  \n",
      " 1   lat                      7869 non-null   float64\n",
      " 2   lon                      7869 non-null   float64\n",
      " 3   distance_from_source     7869 non-null   float64\n",
      " 4   drainage_area            7869 non-null   int64  \n",
      " 5   z_null                   7869 non-null   float64\n",
      " 6   lat_long                 7869 non-null   object \n",
      " 7   met_station_id           7869 non-null   int64  \n",
      " 8   year                     7869 non-null   int64  \n",
      " 9   month                    7869 non-null   int64  \n",
      " 10  stage_max_mean           7865 non-null   float64\n",
      " 11  stage_max_min            7865 non-null   float64\n",
      " 12  stage_max_max            7865 non-null   float64\n",
      " 13  stage_min_mean           7865 non-null   float64\n",
      " 14  stage_min_min            7865 non-null   float64\n",
      " 15  stage_min_max            7865 non-null   float64\n",
      " 16  stage_avg_mean           7865 non-null   float64\n",
      " 17  stage_avg_min            7865 non-null   float64\n",
      " 18  stage_avg_max            7865 non-null   float64\n",
      " 19  ice_thickness_mean       7869 non-null   float64\n",
      " 20  ice_thickness_min        7869 non-null   float64\n",
      " 21  ice_thickness_max        7869 non-null   float64\n",
      " 22  snow_height_mean         7869 non-null   float64\n",
      " 23  snow_height_min          7869 non-null   float64\n",
      " 24  snow_height_max          7869 non-null   float64\n",
      " 25  discharge_mean           2991 non-null   float64\n",
      " 26  discharge_min            2991 non-null   float64\n",
      " 27  discharge_max            2991 non-null   float64\n",
      " 28  temp_mean                4600 non-null   float64\n",
      " 29  temp_min                 4600 non-null   float64\n",
      " 30  temp_max                 4600 non-null   float64\n",
      " 31  ice_thickness_none_mean  7869 non-null   int64  \n",
      " 32  snow_height_none_mean    7869 non-null   int64  \n",
      "dtypes: float64(25), int64(7), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "hydro_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10926 entries, 0 to 10925\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   met_station_id                  10926 non-null  int64  \n",
      " 1   lat                             10926 non-null  float64\n",
      " 2   lon                             10926 non-null  float64\n",
      " 3   z                               10926 non-null  float64\n",
      " 4   lat_long                        10926 non-null  object \n",
      " 5   dist                            10926 non-null  float64\n",
      " 6   year                            10926 non-null  int64  \n",
      " 7   month                           10926 non-null  int64  \n",
      " 8   data_qual                       8801 non-null   float64\n",
      " 9   precipitation_observed          8801 non-null   float64\n",
      " 10  precipitation_corrected         8801 non-null   float64\n",
      " 11  precipitation_corrected_liquid  8801 non-null   float64\n",
      " 12  precipitation_corrected_mixed   8801 non-null   float64\n",
      " 13  precipitation_corrected_solid   8801 non-null   float64\n",
      " 14  sunshine_hours                  7784 non-null   float64\n",
      " 15  date                            10926 non-null  object \n",
      " 16  day                             10926 non-null  int64  \n",
      "dtypes: float64(11), int64(4), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert True, 'стоп'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гидро + Метео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7869, 33)\n",
      "(7869, 47)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7869 entries, 0 to 7868\n",
      "Data columns (total 43 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   hyd_station_id                  7869 non-null   int64  \n",
      " 1   lat                             7869 non-null   float64\n",
      " 2   lon                             7869 non-null   float64\n",
      " 3   distance_from_source            7869 non-null   float64\n",
      " 4   drainage_area                   7869 non-null   int64  \n",
      " 5   z_null                          7869 non-null   float64\n",
      " 6   year                            7869 non-null   int64  \n",
      " 7   month                           7869 non-null   int64  \n",
      " 8   stage_max_mean                  7865 non-null   float64\n",
      " 9   stage_max_min                   7865 non-null   float64\n",
      " 10  stage_max_max                   7865 non-null   float64\n",
      " 11  stage_min_mean                  7865 non-null   float64\n",
      " 12  stage_min_min                   7865 non-null   float64\n",
      " 13  stage_min_max                   7865 non-null   float64\n",
      " 14  stage_avg_mean                  7865 non-null   float64\n",
      " 15  stage_avg_min                   7865 non-null   float64\n",
      " 16  stage_avg_max                   7865 non-null   float64\n",
      " 17  ice_thickness_mean              7869 non-null   float64\n",
      " 18  ice_thickness_min               7869 non-null   float64\n",
      " 19  ice_thickness_max               7869 non-null   float64\n",
      " 20  snow_height_mean                7869 non-null   float64\n",
      " 21  snow_height_min                 7869 non-null   float64\n",
      " 22  snow_height_max                 7869 non-null   float64\n",
      " 23  discharge_mean                  2991 non-null   float64\n",
      " 24  discharge_min                   2991 non-null   float64\n",
      " 25  discharge_max                   2991 non-null   float64\n",
      " 26  temp_mean                       4600 non-null   float64\n",
      " 27  temp_min                        4600 non-null   float64\n",
      " 28  temp_max                        4600 non-null   float64\n",
      " 29  ice_thickness_none_mean         7869 non-null   int64  \n",
      " 30  snow_height_none_mean           7869 non-null   int64  \n",
      " 31  lat_met                         7264 non-null   float64\n",
      " 32  lon_met                         7264 non-null   float64\n",
      " 33  z                               7264 non-null   float64\n",
      " 34  dist                            7264 non-null   float64\n",
      " 35  data_qual                       5718 non-null   float64\n",
      " 36  precipitation_observed          5718 non-null   float64\n",
      " 37  precipitation_corrected         5718 non-null   float64\n",
      " 38  precipitation_corrected_liquid  5718 non-null   float64\n",
      " 39  precipitation_corrected_mixed   5718 non-null   float64\n",
      " 40  precipitation_corrected_solid   5718 non-null   float64\n",
      " 41  sunshine_hours                  4987 non-null   float64\n",
      " 42  day                             7264 non-null   float64\n",
      "dtypes: float64(37), int64(6)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "print(hydro_all.shape)\n",
    "DATA = hydro_all.merge(meteo_all, on = ['met_station_id','year','month'], suffixes=('','_met'), how='left') # day \n",
    "print(DATA.shape)\n",
    "#print(DATA.columns)\n",
    "\n",
    "DATA.drop(['met_station_id', 'date', 'lat_long', 'lat_long_met'], axis=1, inplace=True)\n",
    "\n",
    "#Вот эти возможно позже заюзаем\n",
    "#DATA.drop(['date_1day','date_1month'], axis=1, inplace=True)\n",
    "DATA.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# water_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_codes = hydro_1day.water_code.unique()\n",
    "\n",
    "# def get_ditct_freq(text_codes, code_freq = {}):\n",
    "#     for code in text_codes:\n",
    "#         if str(code) != 'nan':\n",
    "#             if len(code.split(','))== 1:\n",
    "#                 if code in code_freq.keys():\n",
    "#                     code_freq[code] +=1\n",
    "#                 else: code_freq[code] = 1\n",
    "#             else:\n",
    "#                 for x in code.split(','):\n",
    "#                     if x in code_freq.keys():\n",
    "#                         code_freq[x] +=1\n",
    "#                     else: code_freq[x] = 1\n",
    "                    \n",
    "#     return code_freq\n",
    "\n",
    "# water_code_freq = get_ditct_freq(text_codes)            \n",
    "    \n",
    "    \n",
    "# def get_water_code_freq_max_min(x):\n",
    "#     if x is None: return -1\n",
    "#     x = x.strip() \n",
    "#     if len(x.split(',')) == 1: return int(x), water_code_freq[i], -1\n",
    "#     x = x.strip().split(',')\n",
    "#     y = np.array([water_code_freq[i] for i in x])\n",
    "#     return x[np.argmax(y)], x[np.argmin(y)], max(y), min(y)  # Самое частое, самое редкое, частота первого, частота второго\n",
    "\n",
    "\n",
    "# def get_len_water_code(x):\n",
    "#     if x is None:\n",
    "#         return -1\n",
    "#     return len(str(x).split(','))\n",
    "    \n",
    "     \n",
    "# # water_code_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA['len_water_code'] =DATA['water_code'].apply(get_len_water_code)\n",
    "# DATA[['len_water_code', 'water_code']]\n",
    "\n",
    "\n",
    "#TODOs применить  get_water_code_freq_max_min\n",
    "# df_met_day[['latitude', 'longitude']].apply(get_hyd_id,  axis=1)\n",
    "\n",
    "#TODs применить лэйбл кода\n",
    "# refer_water_codes = pd.read_csv(data_dir + 'reference_water_codes.csv') # rwc\n",
    "# refer_water_codes #TODOs заюзать!\n",
    "\n",
    "#DATA.drop(['water_code'], axis=1, inplace=True)  #Извлекли, все что смогли, теперь дропаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA + ExternalData  (Cloud +  Wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_Ext = 'External_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs разбить по метке времени дня"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['latitude', 'longitude', 'time', 'u10', 'v10', 't2m', 'hcc', 'mcc',\n",
      "       'sd', 'tcc', 'tp', 'hyd_station_id', 'year', 'month', 'day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cloud_data = pd.read_csv(data_dir_Ext + 'cloud_data.csv') # hc\n",
    "cloud_data.rename(columns = {'new_hyd_id':'hyd_station_id'}, inplace=True)\n",
    "\n",
    "\n",
    "tmp = cloud_data['time'].apply(lambda x: x.split()[0].split('-'))\n",
    "cloud_data['year'] = tmp.apply(lambda x: int(x[0]))\n",
    "cloud_data['month'] = tmp.apply(lambda x: int(x[1]))\n",
    "cloud_data['day'] = tmp.apply(lambda x: int(x[2]))\n",
    "\n",
    "print(cloud_data.columns)\n",
    "cloud_data = cloud_data.groupby(['hyd_station_id', 'year', 'month'], as_index=False).agg(\n",
    "                                               u10_mean = ('u10', 'mean'),\n",
    "                                               u10_min = ('u10', 'min'),\n",
    "                                               u10_max = ('u10', 'max'),\n",
    "                                               u10_std = ('u10', 'std'),\n",
    "\n",
    "                                               v10_mean = ('v10', 'mean'),\n",
    "                                               v10_min = ('v10', 'min'),\n",
    "                                               v10_max = ('v10', 'max'),\n",
    "                                               v10_std = ('v10', 'std'),\n",
    "\n",
    "                                               tcc_mean = ('tcc', 'mean'),\n",
    "                                               tcc_std = ('tcc', 'std'),\n",
    "                                               tcc_min = ('tcc', 'min'),\n",
    "                                               tcc_max = ('tcc', 'max'),\n",
    "    \n",
    "                                               sd_mean = ('sd', 'mean'),\n",
    "                                               sd_std = ('sd', 'std'),\n",
    "                                               sd_min = ('sd', 'min'),\n",
    "                                               sd_max = ('sd', 'max'),\n",
    "    \n",
    "    \n",
    "                                               mcc_mean = ('mcc', 'mean'),\n",
    "                                               mcc_std = ('mcc', 'std'),\n",
    "                                               mcc_min = ('mcc', 'min'),\n",
    "                                               mcc_max = ('mcc', 'max'),\n",
    "    \n",
    "    \n",
    "                                               tp_mean = ('tp', 'mean'),\n",
    "                                               tp_std = ('tp', 'std'),\n",
    "                                               tp_min = ('tp', 'min'),\n",
    "                                               tp_max = ('tp', 'max'),\n",
    "                                                            )          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['latitude', 'longitude', 'time', 'cc', 't', 'u', 'v', 'hyd_station_id',\n",
      "       'year', 'month', 'day'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyd_station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>cc_mean</th>\n",
       "      <th>cc_min</th>\n",
       "      <th>cc_max</th>\n",
       "      <th>cc_std</th>\n",
       "      <th>t_mean</th>\n",
       "      <th>t_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>t_std</th>\n",
       "      <th>u_mean</th>\n",
       "      <th>u_std</th>\n",
       "      <th>u_min</th>\n",
       "      <th>u_max</th>\n",
       "      <th>v_mean</th>\n",
       "      <th>v_std</th>\n",
       "      <th>v_min</th>\n",
       "      <th>v_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3019</td>\n",
       "      <td>1989</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.406353</td>\n",
       "      <td>251.251600</td>\n",
       "      <td>244.60435</td>\n",
       "      <td>257.91150</td>\n",
       "      <td>5.938624</td>\n",
       "      <td>0.314838</td>\n",
       "      <td>0.933750</td>\n",
       "      <td>-1.029212</td>\n",
       "      <td>1.270499</td>\n",
       "      <td>1.416713</td>\n",
       "      <td>1.571383</td>\n",
       "      <td>-0.710074</td>\n",
       "      <td>3.347969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3019</td>\n",
       "      <td>1989</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264.936162</td>\n",
       "      <td>261.07855</td>\n",
       "      <td>267.98843</td>\n",
       "      <td>2.720983</td>\n",
       "      <td>1.550159</td>\n",
       "      <td>0.917156</td>\n",
       "      <td>0.013884</td>\n",
       "      <td>2.568417</td>\n",
       "      <td>2.018341</td>\n",
       "      <td>0.621657</td>\n",
       "      <td>1.337306</td>\n",
       "      <td>2.789959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3019</td>\n",
       "      <td>1989</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.723625</td>\n",
       "      <td>259.88327</td>\n",
       "      <td>273.15005</td>\n",
       "      <td>5.739940</td>\n",
       "      <td>1.401343</td>\n",
       "      <td>1.445059</td>\n",
       "      <td>-0.896412</td>\n",
       "      <td>2.529823</td>\n",
       "      <td>1.572900</td>\n",
       "      <td>1.599155</td>\n",
       "      <td>-0.483671</td>\n",
       "      <td>3.333082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3019</td>\n",
       "      <td>1989</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>275.934095</td>\n",
       "      <td>273.53903</td>\n",
       "      <td>278.11150</td>\n",
       "      <td>1.732745</td>\n",
       "      <td>1.567814</td>\n",
       "      <td>1.402643</td>\n",
       "      <td>-0.711382</td>\n",
       "      <td>3.428219</td>\n",
       "      <td>-0.467539</td>\n",
       "      <td>1.382491</td>\n",
       "      <td>-2.148702</td>\n",
       "      <td>1.244625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3019</td>\n",
       "      <td>1989</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>282.956690</td>\n",
       "      <td>278.64145</td>\n",
       "      <td>288.53534</td>\n",
       "      <td>4.081835</td>\n",
       "      <td>0.494152</td>\n",
       "      <td>2.348233</td>\n",
       "      <td>-1.942005</td>\n",
       "      <td>3.825744</td>\n",
       "      <td>-1.291499</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>-2.435357</td>\n",
       "      <td>-0.342968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hyd_station_id  year  month   cc_mean  cc_min  cc_max    cc_std  \\\n",
       "0            3019  1989      1  0.170573     0.0     1.0  0.406353   \n",
       "1            3019  1989      2  0.000000     0.0     0.0  0.000000   \n",
       "2            3019  1989      3  0.000000     0.0     0.0  0.000000   \n",
       "3            3019  1989      4  0.000000     0.0     0.0  0.000000   \n",
       "4            3019  1989      5  0.000000     0.0     0.0  0.000000   \n",
       "\n",
       "       t_mean      t_min      t_max     t_std    u_mean     u_std     u_min  \\\n",
       "0  251.251600  244.60435  257.91150  5.938624  0.314838  0.933750 -1.029212   \n",
       "1  264.936162  261.07855  267.98843  2.720983  1.550159  0.917156  0.013884   \n",
       "2  266.723625  259.88327  273.15005  5.739940  1.401343  1.445059 -0.896412   \n",
       "3  275.934095  273.53903  278.11150  1.732745  1.567814  1.402643 -0.711382   \n",
       "4  282.956690  278.64145  288.53534  4.081835  0.494152  2.348233 -1.942005   \n",
       "\n",
       "      u_max    v_mean     v_std     v_min     v_max  \n",
       "0  1.270499  1.416713  1.571383 -0.710074  3.347969  \n",
       "1  2.568417  2.018341  0.621657  1.337306  2.789959  \n",
       "2  2.529823  1.572900  1.599155 -0.483671  3.333082  \n",
       "3  3.428219 -0.467539  1.382491 -2.148702  1.244625  \n",
       "4  3.825744 -1.291499  0.818122 -2.435357 -0.342968  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_data = pd.read_csv(data_dir_Ext + 'wind_data.csv') # hc\n",
    "wind_data.rename(columns = {'new_hyd_id':'hyd_station_id'}, inplace=True)\n",
    "\n",
    "tmp = wind_data['time'].apply(lambda x: x.split()[0].split('-'))\n",
    "wind_data['year'] = tmp.apply(lambda x: int(x[0]))\n",
    "wind_data['month'] = tmp.apply(lambda x: int(x[1]))\n",
    "wind_data['day'] = tmp.apply(lambda x: int(x[2]))\n",
    "\n",
    "print(wind_data.columns)\n",
    "\n",
    "\n",
    "wind_data = wind_data.groupby(['hyd_station_id', 'year', 'month'], as_index=False).agg(\n",
    "                                               cc_mean = ('cc', 'mean'),\n",
    "                                               cc_min = ('cc', 'min'),\n",
    "                                               cc_max = ('cc', 'max'),\n",
    "                                               cc_std = ('cc', 'std'),\n",
    "\n",
    "                                               t_mean = ('t', 'mean'),\n",
    "                                               t_min = ('t', 'min'),\n",
    "                                               t_max = ('t', 'max'),\n",
    "                                               t_std = ('t', 'std'),\n",
    "\n",
    "                                               u_mean = ('u', 'mean'),\n",
    "                                               u_std = ('u', 'std'),\n",
    "                                               u_min = ('u', 'min'),\n",
    "                                               u_max = ('u', 'max'),\n",
    "    \n",
    "                                               v_mean = ('v', 'mean'),\n",
    "                                               v_std = ('v', 'std'),\n",
    "                                               v_min = ('v', 'min'),\n",
    "                                               v_max = ('v', 'max'),\n",
    "                                                            )          \n",
    "\n",
    "\n",
    "\n",
    "wind_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4455 entries, 0 to 4454\n",
      "Data columns (total 43 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   hyd_station_id  4455 non-null   int64  \n",
      " 1   year            4455 non-null   int64  \n",
      " 2   month           4455 non-null   int64  \n",
      " 3   u10_mean        4455 non-null   float64\n",
      " 4   u10_min         4455 non-null   float64\n",
      " 5   u10_max         4455 non-null   float64\n",
      " 6   u10_std         4455 non-null   float64\n",
      " 7   v10_mean        4455 non-null   float64\n",
      " 8   v10_min         4455 non-null   float64\n",
      " 9   v10_max         4455 non-null   float64\n",
      " 10  v10_std         4455 non-null   float64\n",
      " 11  tcc_mean        4455 non-null   float64\n",
      " 12  tcc_std         4455 non-null   float64\n",
      " 13  tcc_min         4455 non-null   float64\n",
      " 14  tcc_max         4455 non-null   float64\n",
      " 15  sd_mean         4455 non-null   float64\n",
      " 16  sd_std          4455 non-null   float64\n",
      " 17  sd_min          4455 non-null   float64\n",
      " 18  sd_max          4455 non-null   float64\n",
      " 19  mcc_mean        4455 non-null   float64\n",
      " 20  mcc_std         4455 non-null   float64\n",
      " 21  mcc_min         4455 non-null   float64\n",
      " 22  mcc_max         4455 non-null   float64\n",
      " 23  tp_mean         4455 non-null   float64\n",
      " 24  tp_std          4455 non-null   float64\n",
      " 25  tp_min          4455 non-null   float64\n",
      " 26  tp_max          4455 non-null   float64\n",
      " 27  cc_mean         4455 non-null   float64\n",
      " 28  cc_min          4455 non-null   float64\n",
      " 29  cc_max          4455 non-null   float64\n",
      " 30  cc_std          4455 non-null   float64\n",
      " 31  t_mean          4455 non-null   float64\n",
      " 32  t_min           4455 non-null   float64\n",
      " 33  t_max           4455 non-null   float64\n",
      " 34  t_std           4455 non-null   float64\n",
      " 35  u_mean          4455 non-null   float64\n",
      " 36  u_std           4455 non-null   float64\n",
      " 37  u_min           4455 non-null   float64\n",
      " 38  u_max           4455 non-null   float64\n",
      " 39  v_mean          4455 non-null   float64\n",
      " 40  v_std           4455 non-null   float64\n",
      " 41  v_min           4455 non-null   float64\n",
      " 42  v_max           4455 non-null   float64\n",
      "dtypes: float64(40), int64(3)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_ext = cloud_data.merge(wind_data, on = ['hyd_station_id', 'year', 'month'], how='inner')\n",
    "DATA_ext.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA.merge(DATA_ext, on = ['hyd_station_id', 'year', 'month'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2718 entries, 0 to 2717\n",
      "Data columns (total 83 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   hyd_station_id                  2718 non-null   int64  \n",
      " 1   lat                             2718 non-null   float64\n",
      " 2   lon                             2718 non-null   float64\n",
      " 3   distance_from_source            2718 non-null   float64\n",
      " 4   drainage_area                   2718 non-null   int64  \n",
      " 5   z_null                          2718 non-null   float64\n",
      " 6   year                            2718 non-null   int64  \n",
      " 7   month                           2718 non-null   int64  \n",
      " 8   stage_max_mean                  2716 non-null   float64\n",
      " 9   stage_max_min                   2716 non-null   float64\n",
      " 10  stage_max_max                   2716 non-null   float64\n",
      " 11  stage_min_mean                  2716 non-null   float64\n",
      " 12  stage_min_min                   2716 non-null   float64\n",
      " 13  stage_min_max                   2716 non-null   float64\n",
      " 14  stage_avg_mean                  2716 non-null   float64\n",
      " 15  stage_avg_min                   2716 non-null   float64\n",
      " 16  stage_avg_max                   2716 non-null   float64\n",
      " 17  ice_thickness_mean              2718 non-null   float64\n",
      " 18  ice_thickness_min               2718 non-null   float64\n",
      " 19  ice_thickness_max               2718 non-null   float64\n",
      " 20  snow_height_mean                2718 non-null   float64\n",
      " 21  snow_height_min                 2718 non-null   float64\n",
      " 22  snow_height_max                 2718 non-null   float64\n",
      " 23  discharge_mean                  1047 non-null   float64\n",
      " 24  discharge_min                   1047 non-null   float64\n",
      " 25  discharge_max                   1047 non-null   float64\n",
      " 26  temp_mean                       718 non-null    float64\n",
      " 27  temp_min                        718 non-null    float64\n",
      " 28  temp_max                        718 non-null    float64\n",
      " 29  ice_thickness_none_mean         2718 non-null   int64  \n",
      " 30  snow_height_none_mean           2718 non-null   int64  \n",
      " 31  lat_met                         2519 non-null   float64\n",
      " 32  lon_met                         2519 non-null   float64\n",
      " 33  z                               2519 non-null   float64\n",
      " 34  dist                            2519 non-null   float64\n",
      " 35  data_qual                       1910 non-null   float64\n",
      " 36  precipitation_observed          1910 non-null   float64\n",
      " 37  precipitation_corrected         1910 non-null   float64\n",
      " 38  precipitation_corrected_liquid  1910 non-null   float64\n",
      " 39  precipitation_corrected_mixed   1910 non-null   float64\n",
      " 40  precipitation_corrected_solid   1910 non-null   float64\n",
      " 41  sunshine_hours                  1739 non-null   float64\n",
      " 42  day                             2519 non-null   float64\n",
      " 43  u10_mean                        2718 non-null   float64\n",
      " 44  u10_min                         2718 non-null   float64\n",
      " 45  u10_max                         2718 non-null   float64\n",
      " 46  u10_std                         2718 non-null   float64\n",
      " 47  v10_mean                        2718 non-null   float64\n",
      " 48  v10_min                         2718 non-null   float64\n",
      " 49  v10_max                         2718 non-null   float64\n",
      " 50  v10_std                         2718 non-null   float64\n",
      " 51  tcc_mean                        2718 non-null   float64\n",
      " 52  tcc_std                         2718 non-null   float64\n",
      " 53  tcc_min                         2718 non-null   float64\n",
      " 54  tcc_max                         2718 non-null   float64\n",
      " 55  sd_mean                         2718 non-null   float64\n",
      " 56  sd_std                          2718 non-null   float64\n",
      " 57  sd_min                          2718 non-null   float64\n",
      " 58  sd_max                          2718 non-null   float64\n",
      " 59  mcc_mean                        2718 non-null   float64\n",
      " 60  mcc_std                         2718 non-null   float64\n",
      " 61  mcc_min                         2718 non-null   float64\n",
      " 62  mcc_max                         2718 non-null   float64\n",
      " 63  tp_mean                         2718 non-null   float64\n",
      " 64  tp_std                          2718 non-null   float64\n",
      " 65  tp_min                          2718 non-null   float64\n",
      " 66  tp_max                          2718 non-null   float64\n",
      " 67  cc_mean                         2718 non-null   float64\n",
      " 68  cc_min                          2718 non-null   float64\n",
      " 69  cc_max                          2718 non-null   float64\n",
      " 70  cc_std                          2718 non-null   float64\n",
      " 71  t_mean                          2718 non-null   float64\n",
      " 72  t_min                           2718 non-null   float64\n",
      " 73  t_max                           2718 non-null   float64\n",
      " 74  t_std                           2718 non-null   float64\n",
      " 75  u_mean                          2718 non-null   float64\n",
      " 76  u_std                           2718 non-null   float64\n",
      " 77  u_min                           2718 non-null   float64\n",
      " 78  u_max                           2718 non-null   float64\n",
      " 79  v_mean                          2718 non-null   float64\n",
      " 80  v_std                           2718 non-null   float64\n",
      " 81  v_min                           2718 non-null   float64\n",
      " 82  v_max                           2718 non-null   float64\n",
      "dtypes: float64(77), int64(6)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_is_none(x):  #Заметим что фичу не надо категоризировать!\n",
    "#     if x is None: return 1\n",
    "#     else: return 0\n",
    "\n",
    "    \n",
    "    \n",
    "# DATA['ice_thickness_is_none'] =DATA['ice_thickness'].apply(check_is_none)\n",
    "# DATA['ice_thickness'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['snow_height_is_none'] =DATA['snow_height'].apply(check_is_none)\n",
    "# DATA['snow_height'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_aver_is_none'] =DATA['ice_crust_aver'].apply(check_is_none)\n",
    "# DATA['ice_crust_aver'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_route_is_none'] =DATA['ice_crust_route'].apply(check_is_none)\n",
    "# DATA['ice_crust_route'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['ice_crust_route_is_none'] =DATA['ice_crust_route'].apply(check_is_none)\n",
    "# DATA['ice_crust_route'].fillna(0, inplace=True)\n",
    "\n",
    "# DATA['snow_saturated_thickness_is_none'] =DATA['snow_saturated_thickness'].apply(check_is_none)\n",
    "# DATA['snow_saturated_thickness'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# ice_thickness\n",
    "# snow_height\n",
    "# snow_saturated_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заполняем средними численные значения\n",
    "\n",
    "sunshine_hours_mean = DATA.sunshine_hours.mean()\n",
    "DATA.sunshine_hours.fillna(sunshine_hours_mean, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#TODOs сделать поумнее заполнение нулей\n",
    "#TODOs разобраться почему у нас наны вообще,мб не везде верный мерджинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns\n",
    "\n",
    "common_columns = ['lat', 'lon', 'z_null'] \n",
    "month_columns = ['stage_max_mean', 'stage_max_min',\n",
    "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
    "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
    "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
    "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
    "       'discharge_mean', 'discharge_min', 'discharge_max',\n",
    "       'ice_thickness_none_mean', 'snow_height_none_mean',\n",
    "       'data_qual', 'precipitation_observed',\n",
    "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
    "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
    "       'sunshine_hours','temp_mean', 'temp_min', 'temp_max',\n",
    "                \n",
    "        #Внешние данные (cloud + wind)\n",
    "        'u10_mean', 'u10_min','u10_max','u10_std','v10_mean','v10_min',\n",
    "        'v10_max', 'v10_std', 'tcc_mean', 'tcc_std', 'tcc_min', 'tcc_max',\n",
    "        'sd_mean', 'sd_std', 'sd_min', 'sd_max', 'mcc_mean', 'mcc_std',         \n",
    "        'mcc_min', 'mcc_max', 'tp_mean', 'tp_std', 'tp_min', 'tp_max', 'cc_mean',\n",
    "        'cc_min', 'cc_max', 'cc_std', 't_mean', 't_min', 't_max', 't_std', 'u_mean',\n",
    "        'u_std', 'u_min', 'u_max', 'v_mean', 'v_std', 'v_min', 'v_max',             \n",
    "                ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>hyd_station_id</th>\n",
       "      <th>day_for_pred</th>\n",
       "      <th>ice_jam</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  hyd_station_id  day_for_pred  ice_jam  M1  M2  M3  M4\n",
       "0  2000            3019             1      0.0   1   2   3   4\n",
       "1  2000            3019             2      0.0   1   2   3   4\n",
       "2  2000            3019             3      0.0   1   2   3   4\n",
       "3  2000            3019             4      0.0   1   2   3   4\n",
       "4  2000            3019             5      0.0   1   2   3   4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_month_key(day):  # Избегаем только месяцев после заторного периода\n",
    "    if day <= 9: return 1, 2, 3, 4\n",
    "    elif day > 9 and day < 39: return 2, 3, 4, 5\n",
    "    else: return 3, 4, 5, 6\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "tmp = train['day'].apply(get_month_key)\n",
    "train['M1'] = tmp.apply(lambda x: x[0])\n",
    "train['M2'] = tmp.apply(lambda x: x[1])\n",
    "train['M3'] = tmp.apply(lambda x: x[2])\n",
    "train['M4'] = tmp.apply(lambda x: x[3])\n",
    "\n",
    "\n",
    "\n",
    "tmp = test['day'].apply(get_month_key)\n",
    "test['M1'] = tmp.apply(lambda x: x[0])\n",
    "test['M2'] = tmp.apply(lambda x: x[1])\n",
    "test['M3'] = tmp.apply(lambda x: x[2])\n",
    "test['M4'] = tmp.apply(lambda x: x[3])\n",
    "\n",
    "\n",
    "#train.head()\n",
    "train.rename(columns = {'station_id':'hyd_station_id', 'day' : 'day_for_pred'}, inplace=True)\n",
    "test.rename(columns = {'station_id':'hyd_station_id', 'day' : 'day_for_pred'}, inplace=True)\n",
    "print(train.columns)\n",
    "train.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hyd_station_id', 'lat', 'lon', 'distance_from_source', 'drainage_area',\n",
       "       'z_null', 'year', 'month', 'stage_max_mean', 'stage_max_min',\n",
       "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
       "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
       "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
       "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
       "       'discharge_mean', 'discharge_min', 'discharge_max', 'temp_mean',\n",
       "       'temp_min', 'temp_max', 'ice_thickness_none_mean',\n",
       "       'snow_height_none_mean', 'lat_met', 'lon_met', 'z', 'dist', 'data_qual',\n",
       "       'precipitation_observed', 'precipitation_corrected',\n",
       "       'precipitation_corrected_liquid', 'precipitation_corrected_mixed',\n",
       "       'precipitation_corrected_solid', 'sunshine_hours', 'day', 'u10_mean',\n",
       "       'u10_min', 'u10_max', 'u10_std', 'v10_mean', 'v10_min', 'v10_max',\n",
       "       'v10_std', 'tcc_mean', 'tcc_std', 'tcc_min', 'tcc_max', 'sd_mean',\n",
       "       'sd_std', 'sd_min', 'sd_max', 'mcc_mean', 'mcc_std', 'mcc_min',\n",
       "       'mcc_max', 'tp_mean', 'tp_std', 'tp_min', 'tp_max', 'cc_mean', 'cc_min',\n",
       "       'cc_max', 'cc_std', 't_mean', 't_min', 't_max', 't_std', 'u_mean',\n",
       "       'u_std', 'u_min', 'u_max', 'v_mean', 'v_std', 'v_min', 'v_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA to TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11277, 8)\n",
      "(2718, 87)\n",
      "(11277, 92)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'temp_mean', 'temp_min', 'temp_max',\n",
      "       'ice_thickness_none_mean', 'snow_height_none_mean', 'lat_met',\n",
      "       'lon_met', 'z', 'dist', 'data_qual', 'precipitation_observed',\n",
      "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
      "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
      "       'sunshine_hours', 'day', 'u10_mean', 'u10_min', 'u10_max', 'u10_std',\n",
      "       'v10_mean', 'v10_min', 'v10_max', 'v10_std', 'tcc_mean', 'tcc_std',\n",
      "       'tcc_min', 'tcc_max', 'sd_mean', 'sd_std', 'sd_min', 'sd_max',\n",
      "       'mcc_mean', 'mcc_std', 'mcc_min', 'mcc_max', 'tp_mean', 'tp_std',\n",
      "       'tp_min', 'tp_max', 'cc_mean', 'cc_min', 'cc_max', 'cc_std', 't_mean',\n",
      "       't_min', 't_max', 't_std', 'u_mean', 'u_std', 'u_min', 'u_max',\n",
      "       'v_mean', 'v_std', 'v_min', 'v_max', 'M2_M1', 'M3_M1', 'M4_M1'],\n",
      "      dtype='object')\n",
      "M1\n",
      "(11277, 162)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       't_max_M2', 't_std_M2', 'u_mean_M2', 'u_std_M2', 'u_min_M2', 'u_max_M2',\n",
      "       'v_mean_M2', 'v_std_M2', 'v_min_M2', 'v_max_M2'],\n",
      "      dtype='object', length=162)\n",
      "M2\n",
      "(11277, 233)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       't_std_M3', 'u_mean_M3', 'u_std_M3', 'u_min_M3', 'u_max_M3',\n",
      "       'v_mean_M3', 'v_std_M3', 'v_min_M3', 'v_max_M3', 'M2_M3'],\n",
      "      dtype='object', length=233)\n",
      "(11277, 305)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'u_mean_M4', 'u_std_M4', 'u_min_M4', 'u_max_M4', 'v_mean_M4',\n",
      "       'v_std_M4', 'v_min_M4', 'v_max_M4', 'M2_M4', 'M3_M4'],\n",
      "      dtype='object', length=305)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11277 entries, 0 to 11276\n",
      "Columns: 305 entries, year to M3_M4\n",
      "dtypes: float64(298), int64(7)\n",
      "memory usage: 26.3 MB\n"
     ]
    }
   ],
   "source": [
    "on = ['year','hyd_station_id', 'M1'] \n",
    "\n",
    "#DATA.rename(columns = {'month':'M1'}, inplace=True)\n",
    "\n",
    "DATA['M1'] = DATA['month']\n",
    "DATA['M2'] = DATA['month']\n",
    "DATA['M3'] = DATA['month']\n",
    "DATA['M4'] = DATA['month']\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(DATA.shape)\n",
    "DATA_train = train.merge(DATA, on = ['year','hyd_station_id', 'M1'], suffixes=('','_M1'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "print('M1')\n",
    "\n",
    "\n",
    "# # DATA.rename(columns = {'M1':'M2'}, inplace=True)\n",
    "month_columns +=  ['year','hyd_station_id' , 'M2']\n",
    "# month_columns\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M2'], suffixes=('','_M2'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "print('M2')\n",
    "month_columns +=  ['M3']\n",
    "# DATA.rename(columns = {'M2':'M3'}, inplace=True)\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M3'], suffixes=('','_M3'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "month_columns +=  [ 'M4']\n",
    "# DATA.rename(columns = {'M3':'M4'}, inplace=True)\n",
    "DATA_train = DATA_train.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M4'], suffixes=('','_M4'), how='left')\n",
    "print(DATA_train.shape)\n",
    "print(DATA_train.columns)\n",
    "\n",
    "DATA_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA to TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3906, 8)\n",
      "(2718, 87)\n",
      "(3906, 92)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null',\n",
      "       'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max',\n",
      "       'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean',\n",
      "       'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean',\n",
      "       'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean',\n",
      "       'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min',\n",
      "       'discharge_max', 'temp_mean', 'temp_min', 'temp_max',\n",
      "       'ice_thickness_none_mean', 'snow_height_none_mean', 'lat_met',\n",
      "       'lon_met', 'z', 'dist', 'data_qual', 'precipitation_observed',\n",
      "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
      "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
      "       'sunshine_hours', 'day', 'u10_mean', 'u10_min', 'u10_max', 'u10_std',\n",
      "       'v10_mean', 'v10_min', 'v10_max', 'v10_std', 'tcc_mean', 'tcc_std',\n",
      "       'tcc_min', 'tcc_max', 'sd_mean', 'sd_std', 'sd_min', 'sd_max',\n",
      "       'mcc_mean', 'mcc_std', 'mcc_min', 'mcc_max', 'tp_mean', 'tp_std',\n",
      "       'tp_min', 'tp_max', 'cc_mean', 'cc_min', 'cc_max', 'cc_std', 't_mean',\n",
      "       't_min', 't_max', 't_std', 'u_mean', 'u_std', 'u_min', 'u_max',\n",
      "       'v_mean', 'v_std', 'v_min', 'v_max', 'M2_M1', 'M3_M1', 'M4_M1'],\n",
      "      dtype='object')\n",
      "M1\n",
      "(3906, 119)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'discharge_max_M2', 'ice_thickness_none_mean_M2',\n",
      "       'snow_height_none_mean_M2', 'data_qual_M2', 'precipitation_observed_M2',\n",
      "       'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2',\n",
      "       'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2',\n",
      "       'sunshine_hours_M2'],\n",
      "      dtype='object', length=119)\n",
      "M2\n",
      "(3906, 147)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'ice_thickness_none_mean_M3', 'snow_height_none_mean_M3',\n",
      "       'data_qual_M3', 'precipitation_observed_M3',\n",
      "       'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3',\n",
      "       'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3',\n",
      "       'sunshine_hours_M3', 'M2_M3'],\n",
      "      dtype='object', length=147)\n",
      "(3906, 176)\n",
      "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
      "       'M4', 'lat', 'lon',\n",
      "       ...\n",
      "       'snow_height_none_mean_M4', 'data_qual_M4', 'precipitation_observed_M4',\n",
      "       'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4',\n",
      "       'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4',\n",
      "       'sunshine_hours_M4', 'M2_M4', 'M3_M4'],\n",
      "      dtype='object', length=176)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3906 entries, 0 to 3905\n",
      "Columns: 176 entries, year to M3_M4\n",
      "dtypes: float64(169), int64(7)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "month_columns = ['stage_max_mean', 'stage_max_min',\n",
    "       'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max',\n",
    "       'stage_avg_mean', 'stage_avg_min', 'stage_avg_max',\n",
    "       'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max',\n",
    "       'snow_height_mean', 'snow_height_min', 'snow_height_max',\n",
    "       'discharge_mean', 'discharge_min', 'discharge_max',\n",
    "       'ice_thickness_none_mean', 'snow_height_none_mean',\n",
    "       'data_qual', 'precipitation_observed',\n",
    "       'precipitation_corrected', 'precipitation_corrected_liquid',\n",
    "       'precipitation_corrected_mixed', 'precipitation_corrected_solid',\n",
    "       'sunshine_hours']\n",
    "\n",
    "\n",
    "print(test.shape)\n",
    "print(DATA.shape)\n",
    "DATA_test = test.merge(DATA, on = ['year','hyd_station_id', 'M1'], suffixes=('','_M1'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "print('M1')\n",
    "\n",
    "\n",
    "# # DATA.rename(columns = {'M1':'M2'}, inplace=True)\n",
    "month_columns +=  ['year','hyd_station_id' , 'M2']\n",
    "# month_columns\n",
    "DATA_test = DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M2'], suffixes=('','_M2'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "print('M2')\n",
    "month_columns +=  ['M3']\n",
    "# DATA.rename(columns = {'M2':'M3'}, inplace=True)\n",
    "DATA_test = DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M3'], suffixes=('','_M3'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "month_columns +=  [ 'M4']\n",
    "# DATA.rename(columns = {'M3':'M4'}, inplace=True)\n",
    "DATA_test= DATA_test.merge(DATA[month_columns], on = ['year','hyd_station_id', 'M4'], suffixes=('','_M4'), how='left')\n",
    "print(DATA_test.shape)\n",
    "print(DATA_test.columns)\n",
    "\n",
    "DATA_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'hyd_station_id', 'day_for_pred', 'ice_jam', 'M1', 'M2', 'M3',\n",
       "       'M4', 'lat', 'lon',\n",
       "       ...\n",
       "       'snow_height_none_mean_M4', 'data_qual_M4', 'precipitation_observed_M4',\n",
       "       'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4',\n",
       "       'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4',\n",
       "       'sunshine_hours_M4', 'M2_M4', 'M3_M4'],\n",
       "      dtype='object', length=176)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train.drop([\"M2_M1\", \"M3_M4\",'M2_M4', \"M3_M1\",\"M2_M3\", \"M4_M1\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_test.drop([\"M2_M1\",\"M2_M4\", \"M3_M4\", \"M3_M1\",\"M2_M3\", \"M4_M1\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in DATA_test.columns:\n",
    "#     print(str('\"'+col+'\"'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока выбросил  из DATA  'day_1month',  \n",
    "#   \"discharge_mean_M4\", \"discharge_min_M4\",\n",
    "# \"discharge_mean_M4\", \"discharge_min_M4\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = [\n",
    "\"year\", \"hyd_station_id\", \"day_for_pred\", \"M1\", \"M2\", \"M3\", \"M4\", \"lat\", \"lon\", \"distance_from_source\", \n",
    "\"drainage_area\", \"z_null\", \"month\", \"stage_max_mean\", \"stage_max_min\", \"stage_max_max\", \"stage_min_mean\", \"stage_min_min\",\n",
    "\"stage_min_max\", \"stage_avg_mean\", \"stage_avg_min\", \"stage_avg_max\", \"ice_thickness_mean\", \"ice_thickness_min\", \n",
    "\"ice_thickness_max\", \"snow_height_mean\", \"snow_height_min\", \"snow_height_max\", \"discharge_mean\", \"discharge_min\",\n",
    "\"discharge_max\", \"temp_mean\", \"temp_min\", \"temp_max\", \"ice_thickness_none_mean\", \"snow_height_none_mean\", \"lat_met\",\n",
    "\"lon_met\", \"z\", \"dist\", \"data_qual\", \"precipitation_observed\", \"precipitation_corrected\", \"precipitation_corrected_liquid\",\n",
    "\"precipitation_corrected_mixed\", \"precipitation_corrected_solid\", \"sunshine_hours\", \"day\", \"u10_mean\", \"u10_min\", \"u10_max\",\n",
    "\"u10_std\", \"v10_mean\", \"v10_min\", \"v10_max\", \"v10_std\", \"tcc_mean\", \"tcc_std\", \"tcc_min\", \"tcc_max\", \"sd_mean\", \"sd_std\",\n",
    "\"sd_min\", \"sd_max\", \"mcc_mean\", \"mcc_std\", \"mcc_min\", \"mcc_max\", \"tp_mean\", \"tp_std\", \"tp_min\", \"tp_max\", \"cc_mean\",\n",
    "\"cc_min\", \"cc_max\", \"cc_std\", \"t_mean\", \"t_min\", \"t_max\", \"t_std\", \"u_mean\", \"u_std\", \"u_min\", \"u_max\", \"v_mean\", \"v_std\",\n",
    "\"v_min\", \"v_max\", \"stage_max_mean_M2\", \"stage_max_min_M2\", \"stage_max_max_M2\", \"stage_min_mean_M2\", \"stage_min_min_M2\",\n",
    "\"stage_min_max_M2\", \"stage_avg_mean_M2\", \"stage_avg_min_M2\", \"stage_avg_max_M2\", \"ice_thickness_mean_M2\",\n",
    "\"ice_thickness_min_M2\", \"ice_thickness_max_M2\", \"snow_height_mean_M2\", \"snow_height_min_M2\", \"snow_height_max_M2\",\n",
    "\"discharge_mean_M2\", \"discharge_min_M2\", \"discharge_max_M2\", \"ice_thickness_none_mean_M2\", \"snow_height_none_mean_M2\",\n",
    "\"data_qual_M2\", \"precipitation_observed_M2\", \"precipitation_corrected_M2\", \"precipitation_corrected_liquid_M2\",\n",
    "\"precipitation_corrected_mixed_M2\", \"precipitation_corrected_solid_M2\", \"sunshine_hours_M2\", \"stage_max_mean_M3\",\n",
    "\"stage_max_min_M3\", \"stage_max_max_M3\", \"stage_min_mean_M3\", \"stage_min_min_M3\", \"stage_min_max_M3\", \"stage_avg_mean_M3\",\n",
    "\"stage_avg_min_M3\", \"stage_avg_max_M3\", \"ice_thickness_mean_M3\", \"ice_thickness_min_M3\", \"ice_thickness_max_M3\",\n",
    "\"snow_height_mean_M3\", \"snow_height_min_M3\", \"snow_height_max_M3\", \"discharge_mean_M3\", \"discharge_min_M3\",\n",
    "\"discharge_max_M3\", \"snow_height_none_mean_M3\", \"data_qual_M3\", \"precipitation_observed_M3\",\n",
    "\"precipitation_corrected_M3\", \"precipitation_corrected_liquid_M3\", \"precipitation_corrected_mixed_M3\",\n",
    "\"precipitation_corrected_solid_M3\", \"sunshine_hours_M3\", \"stage_max_mean_M4\", \"stage_max_min_M4\", \"stage_max_max_M4\",\n",
    "\"stage_min_mean_M4\", \"stage_min_min_M4\", \"stage_min_max_M4\", \"stage_avg_mean_M4\", \"stage_avg_min_M4\", \"stage_avg_max_M4\",\n",
    "\"ice_thickness_mean_M4\", \"ice_thickness_min_M4\", \"ice_thickness_max_M4\", \"snow_height_mean_M4\", \"snow_height_min_M4\",\n",
    "\"snow_height_max_M4\", \"discharge_mean_M4\", \"discharge_min_M4\", \"discharge_max_M4\", \"ice_thickness_none_mean_M4\",\n",
    "\"snow_height_none_mean_M4\", \"data_qual_M4\", \"precipitation_observed_M4\", \"precipitation_corrected_M4\",\n",
    "\"precipitation_corrected_liquid_M4\", \"precipitation_corrected_mixed_M4\", \"precipitation_corrected_solid_M4\",\n",
    "\"sunshine_hours_M4\" ]\n",
    "\n",
    "drop_bad_col =  ['snow_height_none_mean_M4', 'ice_thickness_none_mean_M4','ice_thickness_none_mean',\n",
    "                 'snow_height_none_mean','data_qual', 'ice_thickness_none_mean_M2',\n",
    "                 'snow_height_none_mean_M2','data_qual_M2','ice_thickness_none_mean_M3',\n",
    "                 'snow_height_none_mean_M3', 'temp_min' ,'temp_mean', 'temp_max', 'lat_met']\n",
    "\n",
    "X_columns = [x for x in X_columns if x not in drop_bad_col]\n",
    "\n",
    "y_columns = ['ice_jam']\n",
    "\n",
    "\n",
    "DATA_train.fillna(0,inplace=True)\n",
    "X = DATA_train[X_columns]\n",
    "y = DATA_train['ice_jam']\n",
    "\n",
    "\n",
    "DATA_test.fillna(0,inplace=True)\n",
    "X_test = DATA_test[X_columns]\n",
    "y_test = DATA_test['ice_jam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# index = ['hyd_station_id','year', 'month', 'day', 'lat_met', 'lon_met',]\n",
    "\n",
    "# const_numerical_features = [ 'lat', 'lon', 'distance_from_source'  'drainage_area','z_null', 'z','dist' ]\n",
    "\n",
    "# Числовые признаки\n",
    "numerical_features = [\n",
    "\"day_for_pred\", \"lat\", \"lon\", \"distance_from_source\", \n",
    "\"drainage_area\", \"z_null\", \"stage_max_mean\", \"stage_max_min\", \"stage_max_max\", \"stage_min_mean\", \"stage_min_min\",\n",
    "\"stage_min_max\", \"stage_avg_mean\", \"stage_avg_min\", \"stage_avg_max\", \"ice_thickness_mean\", \"ice_thickness_min\", \n",
    "\"ice_thickness_max\", \"snow_height_mean\", \"snow_height_min\", \"snow_height_max\", \"discharge_mean\", \"discharge_min\",\n",
    "\"discharge_max\", \"temp_mean\", \"temp_min\", \"temp_max\", \"ice_thickness_none_mean\", \"snow_height_none_mean\", \"lat_met\",\n",
    "\"lon_met\", \"z\", \"dist\", \"data_qual\", \"precipitation_observed\", \"precipitation_corrected\", \"precipitation_corrected_liquid\",\n",
    "\"precipitation_corrected_mixed\", \"precipitation_corrected_solid\", \"sunshine_hours\", \"day\", \"u10_mean\", \"u10_min\", \"u10_max\",\n",
    "\"u10_std\", \"v10_mean\", \"v10_min\", \"v10_max\", \"v10_std\", \"tcc_mean\", \"tcc_std\", \"tcc_min\", \"tcc_max\", \"sd_mean\", \"sd_std\",\n",
    "\"sd_min\", \"sd_max\", \"mcc_mean\", \"mcc_std\", \"mcc_min\", \"mcc_max\", \"tp_mean\", \"tp_std\", \"tp_min\", \"tp_max\", \"cc_mean\",\n",
    "\"cc_min\", \"cc_max\", \"cc_std\", \"t_mean\", \"t_min\", \"t_max\", \"t_std\", \"u_mean\", \"u_std\", \"u_min\", \"u_max\", \"v_mean\", \"v_std\",\n",
    "\"v_min\", \"v_max\", \"stage_max_mean_M2\", \"stage_max_min_M2\", \"stage_max_max_M2\", \"stage_min_mean_M2\", \"stage_min_min_M2\",\n",
    "\"stage_min_max_M2\", \"stage_avg_mean_M2\", \"stage_avg_min_M2\", \"stage_avg_max_M2\", \"ice_thickness_mean_M2\",\n",
    "\"ice_thickness_min_M2\", \"ice_thickness_max_M2\", \"snow_height_mean_M2\", \"snow_height_min_M2\", \"snow_height_max_M2\",\n",
    "\"discharge_mean_M2\", \"discharge_min_M2\", \"discharge_max_M2\", \"ice_thickness_none_mean_M2\", \"snow_height_none_mean_M2\",\n",
    "\"data_qual_M2\", \"precipitation_observed_M2\", \"precipitation_corrected_M2\", \"precipitation_corrected_liquid_M2\",\n",
    "\"precipitation_corrected_mixed_M2\", \"precipitation_corrected_solid_M2\", \"sunshine_hours_M2\", \"stage_max_mean_M3\",\n",
    "\"stage_max_min_M3\", \"stage_max_max_M3\", \"stage_min_mean_M3\", \"stage_min_min_M3\", \"stage_min_max_M3\", \"stage_avg_mean_M3\",\n",
    "\"stage_avg_min_M3\", \"stage_avg_max_M3\", \"ice_thickness_mean_M3\", \"ice_thickness_min_M3\", \"ice_thickness_max_M3\",\n",
    "\"snow_height_mean_M3\", \"snow_height_min_M3\", \"snow_height_max_M3\", \"discharge_mean_M3\", \"discharge_min_M3\",\n",
    "\"discharge_max_M3\", \"snow_height_none_mean_M3\", \"data_qual_M3\", \"precipitation_observed_M3\",\n",
    "\"precipitation_corrected_M3\", \"precipitation_corrected_liquid_M3\", \"precipitation_corrected_mixed_M3\",\n",
    "\"precipitation_corrected_solid_M3\", \"sunshine_hours_M3\", \"stage_max_mean_M4\", \"stage_max_min_M4\", \"stage_max_max_M4\",\n",
    "\"stage_min_mean_M4\", \"stage_min_min_M4\", \"stage_min_max_M4\", \"stage_avg_mean_M4\", \"stage_avg_min_M4\", \"stage_avg_max_M4\",\n",
    "\"ice_thickness_mean_M4\", \"ice_thickness_min_M4\", \"ice_thickness_max_M4\", \"snow_height_mean_M4\", \"snow_height_min_M4\",\n",
    "\"snow_height_max_M4\", \"discharge_mean_M4\", \"discharge_min_M4\", \"discharge_max_M4\", \"ice_thickness_none_mean_M4\",\n",
    "\"snow_height_none_mean_M4\", \"data_qual_M4\", \"precipitation_observed_M4\", \"precipitation_corrected_M4\",\n",
    "\"precipitation_corrected_liquid_M4\", \"precipitation_corrected_mixed_M4\", \"precipitation_corrected_solid_M4\",\n",
    "\"sunshine_hours_M4\"       \n",
    "]\n",
    "\n",
    "\n",
    "numerical_features = [x for x in numerical_features if x not in drop_bad_col]\n",
    "\n",
    "# Категориальные признаки\n",
    "categorical_features = [ \"year\", \"hyd_station_id\", \"month\",  \"M1\", \"M2\", \"M3\", \"M4\" ]\n",
    "\n",
    "\n",
    "DATA_train.fillna(0,inplace=True)\n",
    "X = DATA_train[X_columns]\n",
    "y = DATA_train['ice_jam']\n",
    "\n",
    "\n",
    "DATA_test.fillna(0,inplace=True)\n",
    "X_test = DATA_test[X_columns]\n",
    "y_test = DATA_test['ice_jam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lon_met'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dro_col = [29, 30, 31]\n",
    "# for c in dro_col:\n",
    "#     print(X_columns[c+3])\n",
    "    \n",
    "X_columns[29+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_train['lon_met'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'snow_height_none_mean_M4' 'ice_thickness_none_mean_M4'\n",
    "# ice_thickness_none_mean\n",
    "# snow_height_none_mean\n",
    "# data_qual\n",
    "# ice_thickness_none_mean_M2\n",
    "# snow_height_none_mean_M2\n",
    "# data_qual_M2\n",
    "# ice_thickness_none_mean_M3\n",
    "# snow_height_none_mean_M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11277, 299)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(DATA_train.shape)\n",
    "\n",
    "#DATA_train_new = SelectKBest(chi2, k=100).fit_transform(DATA_train.drop('ice_jam', axis=1), DATA_train['ice_jam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3ead3957-5b05-4e04-876b-25c5d94ef10d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3ead3957-5b05-4e04-876b-25c5d94ef10d\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('data_transformer',\n",
       "                                  ColumnTransformer(transformers=[('numerical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    StandardScaler()),\n",
       "                                                                                   ('filter',\n",
       "                                                                                    SelectKBest(k=110))]),\n",
       "                                                                   ['day_for_pred',\n",
       "                                                                    'M1', 'M2',\n",
       "                                                                    'M3', 'M4',\n",
       "                                                                    'lat',\n",
       "                                                                    'lon',\n",
       "                                                                    'distance_from_source',\n",
       "                                                                    'drainage_area',\n",
       "                                                                    'z_null',\n",
       "                                                                    'month',\n",
       "                                                                    'stage_max_mean',\n",
       "                                                                    'stage_max_min',\n",
       "                                                                    'stage_max_max',\n",
       "                                                                    'stage_...\n",
       "                                                                    'snow_height_mean',\n",
       "                                                                    'snow_height_min',\n",
       "                                                                    'snow_height_max',\n",
       "                                                                    'discharge_mean',\n",
       "                                                                    'discharge_min',\n",
       "                                                                    'discharge_max',\n",
       "                                                                    'lon_met', ...]),\n",
       "                                                                  ('categorical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   ('onehot',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                   ['year',\n",
       "                                                                    'hyd_station_id'])]))])),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
       "                                    random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a2a38806-11ee-498f-9770-bd0580e15e5f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a2a38806-11ee-498f-9770-bd0580e15e5f\">preprocessor: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('data_transformer',\n",
       "                 ColumnTransformer(transformers=[('numerical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('filter',\n",
       "                                                                   SelectKBest(k=110))]),\n",
       "                                                  ['day_for_pred', 'M1', 'M2',\n",
       "                                                   'M3', 'M4', 'lat', 'lon',\n",
       "                                                   'distance_from_source',\n",
       "                                                   'drainage_area', 'z_null',\n",
       "                                                   'month', 'stage_max_mean',\n",
       "                                                   'stage_max_min',\n",
       "                                                   'stage_max_max',\n",
       "                                                   'stage_min_mean',\n",
       "                                                   'stage_min_min',\n",
       "                                                   'stage...\n",
       "                                                   'stage_avg_min',\n",
       "                                                   'stage_avg_max',\n",
       "                                                   'ice_thickness_mean',\n",
       "                                                   'ice_thickness_min',\n",
       "                                                   'ice_thickness_max',\n",
       "                                                   'snow_height_mean',\n",
       "                                                   'snow_height_min',\n",
       "                                                   'snow_height_max',\n",
       "                                                   'discharge_mean',\n",
       "                                                   'discharge_min',\n",
       "                                                   'discharge_max', 'lon_met', ...]),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['year',\n",
       "                                                   'hyd_station_id'])]))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e1ca8afe-fbef-45f1-9bb8-0ec537e7d187\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e1ca8afe-fbef-45f1-9bb8-0ec537e7d187\">data_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('numerical',\n",
       "                                 Pipeline(steps=[('imputer', StandardScaler()),\n",
       "                                                 ('filter',\n",
       "                                                  SelectKBest(k=110))]),\n",
       "                                 ['day_for_pred', 'M1', 'M2', 'M3', 'M4', 'lat',\n",
       "                                  'lon', 'distance_from_source',\n",
       "                                  'drainage_area', 'z_null', 'month',\n",
       "                                  'stage_max_mean', 'stage_max_min',\n",
       "                                  'stage_max_max', 'stage_min_mean',\n",
       "                                  'stage_min_min', 'stage_min_max',\n",
       "                                  'stage_avg_mean', 'stage_av...\n",
       "                                  'stage_avg_mean', 'stage_avg_min',\n",
       "                                  'stage_avg_max', 'ice_thickness_mean',\n",
       "                                  'ice_thickness_min', 'ice_thickness_max',\n",
       "                                  'snow_height_mean', 'snow_height_min',\n",
       "                                  'snow_height_max', 'discharge_mean',\n",
       "                                  'discharge_min', 'discharge_max', 'lon_met', ...]),\n",
       "                                ('categorical',\n",
       "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['year', 'hyd_station_id'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"471fafe5-a146-4258-a243-dcce5c28620f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"471fafe5-a146-4258-a243-dcce5c28620f\">numerical</label><div class=\"sk-toggleable__content\"><pre>['day_for_pred', 'M1', 'M2', 'M3', 'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null', 'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean', 'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean', 'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min', 'discharge_max', 'lon_met', 'z', 'dist', 'precipitation_observed', 'precipitation_corrected', 'precipitation_corrected_liquid', 'precipitation_corrected_mixed', 'precipitation_corrected_solid', 'sunshine_hours', 'day', 'u10_mean', 'u10_min', 'u10_max', 'u10_std', 'v10_mean', 'v10_min', 'v10_max', 'v10_std', 'tcc_mean', 'tcc_std', 'tcc_min', 'tcc_max', 'sd_mean', 'sd_std', 'sd_min', 'sd_max', 'mcc_mean', 'mcc_std', 'mcc_min', 'mcc_max', 'tp_mean', 'tp_std', 'tp_min', 'tp_max', 'cc_mean', 'cc_min', 'cc_max', 'cc_std', 't_mean', 't_min', 't_max', 't_std', 'u_mean', 'u_std', 'u_min', 'u_max', 'v_mean', 'v_std', 'v_min', 'v_max', 'stage_max_mean_M2', 'stage_max_min_M2', 'stage_max_max_M2', 'stage_min_mean_M2', 'stage_min_min_M2', 'stage_min_max_M2', 'stage_avg_mean_M2', 'stage_avg_min_M2', 'stage_avg_max_M2', 'ice_thickness_mean_M2', 'ice_thickness_min_M2', 'ice_thickness_max_M2', 'snow_height_mean_M2', 'snow_height_min_M2', 'snow_height_max_M2', 'discharge_mean_M2', 'discharge_min_M2', 'discharge_max_M2', 'precipitation_observed_M2', 'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2', 'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2', 'sunshine_hours_M2', 'stage_max_mean_M3', 'stage_max_min_M3', 'stage_max_max_M3', 'stage_min_mean_M3', 'stage_min_min_M3', 'stage_min_max_M3', 'stage_avg_mean_M3', 'stage_avg_min_M3', 'stage_avg_max_M3', 'ice_thickness_mean_M3', 'ice_thickness_min_M3', 'ice_thickness_max_M3', 'snow_height_mean_M3', 'snow_height_min_M3', 'snow_height_max_M3', 'discharge_mean_M3', 'discharge_min_M3', 'discharge_max_M3', 'data_qual_M3', 'precipitation_observed_M3', 'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3', 'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3', 'sunshine_hours_M3', 'stage_max_mean_M4', 'stage_max_min_M4', 'stage_max_max_M4', 'stage_min_mean_M4', 'stage_min_min_M4', 'stage_min_max_M4', 'stage_avg_mean_M4', 'stage_avg_min_M4', 'stage_avg_max_M4', 'ice_thickness_mean_M4', 'ice_thickness_min_M4', 'ice_thickness_max_M4', 'snow_height_mean_M4', 'snow_height_min_M4', 'snow_height_max_M4', 'discharge_mean_M4', 'discharge_min_M4', 'discharge_max_M4', 'data_qual_M4', 'precipitation_observed_M4', 'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4', 'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4', 'sunshine_hours_M4']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0d74923c-13c3-4d53-91ea-60e97c56b556\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0d74923c-13c3-4d53-91ea-60e97c56b556\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ef1eaf49-ff78-481a-8d3c-19574feca6ce\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ef1eaf49-ff78-481a-8d3c-19574feca6ce\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=110)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1510941c-bb76-4758-90e1-9e584d90bca8\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1510941c-bb76-4758-90e1-9e584d90bca8\">numerical_pca</label><div class=\"sk-toggleable__content\"><pre>['day_for_pred', 'M1', 'M2', 'M3', 'M4', 'lat', 'lon', 'distance_from_source', 'drainage_area', 'z_null', 'month', 'stage_max_mean', 'stage_max_min', 'stage_max_max', 'stage_min_mean', 'stage_min_min', 'stage_min_max', 'stage_avg_mean', 'stage_avg_min', 'stage_avg_max', 'ice_thickness_mean', 'ice_thickness_min', 'ice_thickness_max', 'snow_height_mean', 'snow_height_min', 'snow_height_max', 'discharge_mean', 'discharge_min', 'discharge_max', 'lon_met', 'z', 'dist', 'precipitation_observed', 'precipitation_corrected', 'precipitation_corrected_liquid', 'precipitation_corrected_mixed', 'precipitation_corrected_solid', 'sunshine_hours', 'day', 'u10_mean', 'u10_min', 'u10_max', 'u10_std', 'v10_mean', 'v10_min', 'v10_max', 'v10_std', 'tcc_mean', 'tcc_std', 'tcc_min', 'tcc_max', 'sd_mean', 'sd_std', 'sd_min', 'sd_max', 'mcc_mean', 'mcc_std', 'mcc_min', 'mcc_max', 'tp_mean', 'tp_std', 'tp_min', 'tp_max', 'cc_mean', 'cc_min', 'cc_max', 'cc_std', 't_mean', 't_min', 't_max', 't_std', 'u_mean', 'u_std', 'u_min', 'u_max', 'v_mean', 'v_std', 'v_min', 'v_max', 'stage_max_mean_M2', 'stage_max_min_M2', 'stage_max_max_M2', 'stage_min_mean_M2', 'stage_min_min_M2', 'stage_min_max_M2', 'stage_avg_mean_M2', 'stage_avg_min_M2', 'stage_avg_max_M2', 'ice_thickness_mean_M2', 'ice_thickness_min_M2', 'ice_thickness_max_M2', 'snow_height_mean_M2', 'snow_height_min_M2', 'snow_height_max_M2', 'discharge_mean_M2', 'discharge_min_M2', 'discharge_max_M2', 'precipitation_observed_M2', 'precipitation_corrected_M2', 'precipitation_corrected_liquid_M2', 'precipitation_corrected_mixed_M2', 'precipitation_corrected_solid_M2', 'sunshine_hours_M2', 'stage_max_mean_M3', 'stage_max_min_M3', 'stage_max_max_M3', 'stage_min_mean_M3', 'stage_min_min_M3', 'stage_min_max_M3', 'stage_avg_mean_M3', 'stage_avg_min_M3', 'stage_avg_max_M3', 'ice_thickness_mean_M3', 'ice_thickness_min_M3', 'ice_thickness_max_M3', 'snow_height_mean_M3', 'snow_height_min_M3', 'snow_height_max_M3', 'discharge_mean_M3', 'discharge_min_M3', 'discharge_max_M3', 'data_qual_M3', 'precipitation_observed_M3', 'precipitation_corrected_M3', 'precipitation_corrected_liquid_M3', 'precipitation_corrected_mixed_M3', 'precipitation_corrected_solid_M3', 'sunshine_hours_M3', 'stage_max_mean_M4', 'stage_max_min_M4', 'stage_max_max_M4', 'stage_min_mean_M4', 'stage_min_min_M4', 'stage_min_max_M4', 'stage_avg_mean_M4', 'stage_avg_min_M4', 'stage_avg_max_M4', 'ice_thickness_mean_M4', 'ice_thickness_min_M4', 'ice_thickness_max_M4', 'snow_height_mean_M4', 'snow_height_min_M4', 'snow_height_max_M4', 'discharge_mean_M4', 'discharge_min_M4', 'discharge_max_M4', 'data_qual_M4', 'precipitation_observed_M4', 'precipitation_corrected_M4', 'precipitation_corrected_liquid_M4', 'precipitation_corrected_mixed_M4', 'precipitation_corrected_solid_M4', 'sunshine_hours_M4']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e59ccb94-8ee6-4882-b7a7-f312a73e1b70\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e59ccb94-8ee6-4882-b7a7-f312a73e1b70\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ffb8043c-037e-43f2-9de4-e4712e0e85e6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ffb8043c-037e-43f2-9de4-e4712e0e85e6\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=8)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2aa7e665-8f49-42b9-b2da-b9df7bcc9152\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2aa7e665-8f49-42b9-b2da-b9df7bcc9152\">categorical</label><div class=\"sk-toggleable__content\"><pre>['year', 'hyd_station_id']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c5c1dc6e-46ad-4fc2-9328-7e3178489a2d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c5c1dc6e-46ad-4fc2-9328-7e3178489a2d\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0d8004fc-ff88-4281-ac46-d79f44b33aa1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0d8004fc-ff88-4281-ac46-d79f44b33aa1\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f84c4d2d-a1e3-41bb-bae1-007d4d865a11\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f84c4d2d-a1e3-41bb-bae1-007d4d865a11\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight='balanced', n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('data_transformer',\n",
       "                                  ColumnTransformer(transformers=[('numerical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    StandardScaler()),\n",
       "                                                                                   ('filter',\n",
       "                                                                                    SelectKBest(k=110))]),\n",
       "                                                                   ['day_for_pred',\n",
       "                                                                    'M1', 'M2',\n",
       "                                                                    'M3', 'M4',\n",
       "                                                                    'lat',\n",
       "                                                                    'lon',\n",
       "                                                                    'distance_from_source',\n",
       "                                                                    'drainage_area',\n",
       "                                                                    'z_null',\n",
       "                                                                    'month',\n",
       "                                                                    'stage_max_mean',\n",
       "                                                                    'stage_max_min',\n",
       "                                                                    'stage_max_max',\n",
       "                                                                    'stage_...\n",
       "                                                                    'snow_height_mean',\n",
       "                                                                    'snow_height_min',\n",
       "                                                                    'snow_height_max',\n",
       "                                                                    'discharge_mean',\n",
       "                                                                    'discharge_min',\n",
       "                                                                    'discharge_max',\n",
       "                                                                    'lon_met', ...]),\n",
       "                                                                  ('categorical',\n",
       "                                                                   Pipeline(steps=[('imputer',\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   ('onehot',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                   ['year',\n",
       "                                                                    'hyd_station_id'])]))])),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
       "                                    random_state=42))])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем SimpleImputer и будем искать различные скейлеры с помощью GridSearchCV\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", StandardScaler()), (\"filter\", SelectKBest(f_classif, k=110))]  # StandardScaler()\n",
    ")\n",
    "\n",
    "\n",
    "numerical_transformer_pca = Pipeline(\n",
    "    steps=[(\"imputer\", StandardScaler()), (\"PCA\", PCA(n_components= 8))]  # StandardScaler()\n",
    ")\n",
    "\n",
    "# Применение SimpleImputer, а затем OneHotEncoder\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Собираем воедино трансформеры для числовых и категориальных признаков\n",
    "data_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", numerical_transformer, numerical_features),\n",
    "        (\"numerical_pca\", numerical_transformer_pca, numerical_features),\n",
    "        (\"categorical\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создание конвейера препроцессора, который сначала преобразует данные и затем применяет PCA.\n",
    "preprocessor = Pipeline(steps=[(\"data_transformer\", data_transformer)])\n",
    "#  ('reduce_dim', PCA())])\n",
    "\n",
    "# we are using LinearRegression here\n",
    "classifier = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        #(\"classifier\", LinearRegression(copy_X=True, fit_intercept=True, normalize=True, n_jobs=-1),\n",
    "#         (\"classifier\", RandomForestClassifier(random_state=42, min_samples_split=2, min_samples_leaf = 5,\n",
    "#                                              max_depth=6, n_estimators=90, verbose=True,  n_jobs=-1),\n",
    "#         (\"classifier\", ExtraTreesClassifier(random_state=42, min_samples_split=2, min_samples_leaf = 5,\n",
    "#                                             max_depth=6, n_estimators=90, verbose=True,  n_jobs=-1, class_weight='balanced')\n",
    "        (\"classifier\", LogisticRegression(random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "           \n",
    "        \n",
    "\n",
    "         \n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    11139\n",
       "1.0      138\n",
       "Name: ice_jam, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Лучшие параметры модели:  {'classifier__C': 10, 'classifier__class_weight': 'balanced', 'preprocessor__data_transformer__numerical__filter__k': 20, 'preprocessor__data_transformer__numerical_pca__PCA__n_components': 10}\n",
      "\n",
      "\n",
      "Метрика f1:  0.05592223526155441\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# classifier = Pipeline(\n",
    "#     steps=[\n",
    "#     (\"preprocessor\", preprocessor),\n",
    "#     (\"classifier\", RandomForestRegressor(n_jobs=-1, min_samples_split=3, max_depth=4, n_estimators=200, verbose=True))])\n",
    "\n",
    "model = classifier  \n",
    "parameters = {\n",
    "#     'classifier__min_samples_leaf':[5],\n",
    "#     'classifier__max_depth':[3, 6, 5],\n",
    "#     'classifier__n_estimators':[10, 50, 90, 100,], # 195\n",
    "#     'classifier__min_samples_split':[2, 3],\n",
    "    \n",
    "    \n",
    "    'classifier__class_weight':['balanced'],\n",
    "    'classifier__C':[100, 10, 1, 1/5, 1/10,],\n",
    "    \n",
    "    'preprocessor__data_transformer__numerical__filter__k':[15, 20, 25],\n",
    "    'preprocessor__data_transformer__numerical_pca__PCA__n_components':[5, 10, 20, 25, 30, 35, 40],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(model, parameters, cv=5, n_jobs=-1, scoring= 'f1')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Лучшие параметры модели: \", grid.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Метрика f1: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.016872637393890774\n",
    "# 0.006504065040650407\n",
    "# Метрика f1:  0.009329446064139942 \n",
    "# 0.009504950495049506\n",
    "\n",
    "# 0.03336753574432296\n",
    "\n",
    "# Метрика f1:  0.12248498093212705    K = 30 \n",
    "\n",
    "# 0.1409311910488969  K = 40 \n",
    "\n",
    "# Метрика f1:  0.05592223526155441   PCA__n_components  k =  20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = grid.predict(X_test)\n",
    "sum(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_col = ['year', 'station_id', 'day', 'ice_jam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>3019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>1997</td>\n",
       "      <td>3027</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>1997</td>\n",
       "      <td>3027</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>1997</td>\n",
       "      <td>3027</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <td>1997</td>\n",
       "      <td>3027</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>1997</td>\n",
       "      <td>3027</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3906 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  station_id  day  ice_jam\n",
       "0     2001        3019    0      0.0\n",
       "1     2001        3019    1      0.0\n",
       "2     2001        3019    2      0.0\n",
       "3     2001        3019    3      0.0\n",
       "4     2001        3019    4      0.0\n",
       "...    ...         ...  ...      ...\n",
       "3901  1997        3027   39      1.0\n",
       "3902  1997        3027   40      1.0\n",
       "3903  1997        3027   41      1.0\n",
       "3904  1997        3027   42      1.0\n",
       "3905  1997        3027   43      1.0\n",
       "\n",
       "[3906 rows x 4 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('test_task_1_random.csv') \n",
    "submission['ice_jam'] = preds*1\n",
    "submission.to_csv('first_sub_LR_PCA.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2685\n",
       "0.0    1221\n",
       "Name: ice_jam, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.ice_jam.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "#Зададим размеры графиков\n",
    "plt.rcParams.update({'figure.figsize': (5, 5)})\n",
    "plt.rcParams.update({'font.size': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6dc7df950c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfull_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m  \u001b[1;31m# # get importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfull_importance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_importance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "full_importance = model.named_steps['classifier'].feature_importances_  # # get importance\n",
    "\n",
    "full_importance, X_cols = zip(*sorted(zip(full_importance, X.columns)))\n",
    "\n",
    "pyplot.barh([x for x in X_cols[:25]], full_importance[:25])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим годы на train и test сбалансированно по суммарной продолжительности заторных событий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Суммарное количество заторов в году\n",
    "jams_by_year = DATA_train.groupby('year').sum()['ice_jam'].to_frame().reset_index()\n",
    "jams_by_year['ice_jam'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на бины по квартилям\n",
    "bins = [-1, 3, 5, 8, 16]\n",
    "jams_by_year['ice_jam_bins'] = pd.cut(jams_by_year['ice_jam'], bins)\n",
    "X_length = jams_by_year[['year', 'ice_jam']]\n",
    "y_length = jams_by_year['ice_jam_bins']\n",
    "y_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим годы на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_length, \n",
    "                                                    y_length, \n",
    "                                                    test_size=0.3,  \n",
    "                                                    stratify=y_length, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи из гидроданных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Мы не можем использовать данные из будущего: всё, что происходит после заторного периода, относится уже к следующему году.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Внесем не темпоральные данные\n",
    "# # DATA = pd.merge(DATA, hydro_coord[['hyd_station_id', \n",
    "# #                                 'distance_from_source', \n",
    "# #                                 'drainage_area', \n",
    "# #                                 'z_null']], on='hyd_station_id', how='left')\n",
    "\n",
    "# # Возьмем также данные из ежедневных наблюдений\n",
    "# hydro_1day = pd.read_csv(data_dir + 'hydro_1day.csv',\n",
    "#                    parse_dates=['date'])\n",
    "# hydro_1day.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydro_1day['station_id'] = hydro_1day.station_id.astype(int)\n",
    "# meteo_1day['station_id'] = meteo_1day['station_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ice_jam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1996</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1991</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2006</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1999</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2008</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  ice_jam\n",
       "6   1992      1.0\n",
       "21  2015     11.0\n",
       "23  2017     11.0\n",
       "9   1996      1.0\n",
       "18  2010      0.0\n",
       "25  2019      6.0\n",
       "2   1987      8.0\n",
       "4   1990      4.0\n",
       "5   1991      2.0\n",
       "12  2000      4.0\n",
       "14  2006      3.0\n",
       "11  1999      5.0\n",
       "17  2009      8.0\n",
       "20  2014      5.0\n",
       "16  2008      2.0\n",
       "3   1988      5.0\n",
       "1   1986      9.0\n",
       "0   1985      8.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meteo_1day.head()\n",
    "# hydro_1day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydro_1day = hydro_1day.merge(meteo_1day, on=['station_id', 'year', 'month', 'day'], how='inner')\n",
    "# hydro_1day = hydro_1day.fillna(0)\n",
    "# hydro_1day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скорректируем год, в который доступно наблюдение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Определим для наблюдения год, в который оно доступно\n",
    "\n",
    "# # Маска-окно между заторным периодом и концом года\n",
    "# def after_jam_window(row, local=False):\n",
    "#     if local:\n",
    "#         month = row.month_local\n",
    "#         day = row.date_local.day\n",
    "#     else:\n",
    "#         month = row.month\n",
    "#         day = row.date.day\n",
    "#     return (((month == 6) and (day > 3))\n",
    "#             or (month in [7, 8, 9, 10, 11, 12]))\n",
    "\n",
    "# # Год относительно бизнес-логики\n",
    "# def target_year(row, local=False):\n",
    "#     if local:\n",
    "#         year = row.year_local\n",
    "#     else:\n",
    "#         year = row.year\n",
    "#     if after_jam_window(row):\n",
    "#         return year + 1\n",
    "#     else:\n",
    "#         return year\n",
    "    \n",
    "# # hydro_1day['target_year'] = hydro_1day.apply(target_year, axis=1)\n",
    "\n",
    "# # # Календарный год и день больше не нужны\n",
    "# # hydro_1day.drop(columns=['year', 'date', 'day'], axis=1, inplace=True)\n",
    "\n",
    "# DATA['target_year'] = hydro_1day.apply(target_year, axis=1)\n",
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гидро – сделаем ресэмплинг по месяцу\n",
    "\n",
    "index = ['station_id', 'month', 'target_year']\n",
    "\n",
    "hydro_1day_mean = hydro_1day.groupby(index).mean().add_prefix('mean_').reset_index()\n",
    "hydro_1day_max = hydro_1day.groupby(index).max().add_prefix('max_').reset_index()\n",
    "hydro_1day_min = hydro_1day.groupby(index).min().add_prefix('min_').reset_index()\n",
    "hydro_1day_std = hydro_1day.groupby(index).std().add_prefix('std_').reset_index()\n",
    "data_frames = [hydro_1day_mean, hydro_1day_max, hydro_1day_min, hydro_1day_std]\n",
    "\n",
    "hydro_monthly = pd.concat(data_frames, axis=1)\n",
    "hydro_monthly = hydro_monthly.loc[:,~hydro_monthly.columns.duplicated()]\n",
    "hydro_monthly.sort_values(index).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    station, target_year = df.name\n",
    "    result = pd.DataFrame()\n",
    "    for month, mdf in df.groupby('month'):\n",
    "        m_feats = mdf[df.columns[4:]].add_prefix(str(month) + '_').reset_index(drop=True)\n",
    "        result = pd.concat([result, m_feats], axis=1)\n",
    "    return result.reset_index(drop=True)\n",
    "        \n",
    "hydro_features = hydro_monthly.groupby(['station_id', 'target_year']).apply(make_features)\n",
    "hydro_features = hydro_features.reset_index(level=2, drop=True).reset_index()\n",
    "hydro_features.dropna(how='all', axis=1, inplace=True)\n",
    "hydro_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи в основной датасет\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Важно: merge делаем по target_year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.merge(DATA, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = DATA.columns.to_list()\n",
    "DATA = DATA[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]\n",
    "DATA.dropna(how='any',inplace=True)\n",
    "DATA.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ids, data, target = DATA[DATA.columns[:4]], DATA[DATA.columns[4:-1]], DATA[DATA.columns[-1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "transformed_data = scaler.transform(data)\n",
    "norm_df = pd.concat([ids, pd.DataFrame(transformed_data, columns = DATA.columns[4:-1]), target], axis=1)\n",
    "norm_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на трейн и тест исходя из target_year\n",
    "\n",
    "test = norm_df[~norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "train = norm_df[norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "\n",
    "# target_year больше не нужна\n",
    "\n",
    "test.drop(columns=['target_year'], inplace=True)\n",
    "train.drop(columns=['target_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поделим данные на предикторы и таргет\n",
    "\n",
    "X_train, y_train = train.iloc[:, :-1], train.ice_jam\n",
    "X_test, y_test = test.iloc[:, :-1], test.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  discharge_mean_M4 дропаем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальное разбиение фичей на группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Удаляем дубли\n",
    "\n",
    "# clear_DATA = DATA[X_columns+y_columns].copy()\n",
    "\n",
    "# print('До',clear_DATA.shape)\n",
    "# clear_DATA.drop_duplicates(inplace=True)\n",
    "# print('После', clear_DATA.shape)\n",
    "\n",
    "\n",
    "# X = clear_DATA[X_columns]\n",
    "# y = clear_DATA['MarkPercent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение классификатора и подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Pipeline(\n",
    "#     steps=[\n",
    "#     (\"preprocessor\", preprocessor),\n",
    "#     (\"classifier\", RandomForestRegressor(n_jobs=-1, min_samples_split=3, max_depth=4, n_estimators=200, verbose=True))])\n",
    "\n",
    "model = classifier  \n",
    "parameters = {\n",
    "    'classifier__min_samples_leaf':[11],\n",
    "    'classifier__max_depth':[7],\n",
    "    'classifier__n_estimators':[155], # 195\n",
    "    'classifier__min_samples_split':[4]\n",
    "\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=5, n_jobs=-1, scoring= 'neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Лучшие параметры модели: \", grid.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Метрика RMSE: \", abs(grid.best_score_)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "full_importance = model.named_steps['classifier'].feature_importances_  # # get importance\n",
    "\n",
    "full_importance, X_cols = zip(*sorted(zip(full_importance, X.columns)))\n",
    "\n",
    "pyplot.barh([x for x in X_cols[:15]], full_importance[:15])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим простой классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lsvc = SVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "selector = RFE(LogisticRegression(max_iter=5000),\n",
    "               n_features_to_select=200, step=10).fit(X_train, y_train)\n",
    "X_reduced = selector.transform(X_train)\n",
    "X_reduced_test = selector.transform(X_test)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "parameters = {'pca__n_components': list(range(400,500,50)), 'svc__kernel':('linear', 'rbf'), 'svc__C':[1,10]}\n",
    "clf = Pipeline([('pca', pca), ('svc', SVC(class_weight='balanced'))])\n",
    "GS = GridSearchCV(clf, parameters, scoring='f1_macro')\n",
    "GS.fit(X_train, y_train)\n",
    "print(GS.cv_results_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = GS.predict(X_test)\n",
    "recall_score(y_test, prediction), \\\n",
    "    precision_score(y_test, prediction), f1_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "#test_real = pd.read_csv(data_dir + '../test_SECRET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_real, hydro_coord[['station_id', \n",
    "                                'distance_from_source', \n",
    "                                'drainage_area', \n",
    "                                'z_null']], on='station_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'year' in hydro_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_X, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = test_X.columns.to_list()\n",
    "test_X = test_X[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, data, target = test_X[test_X.columns[:4]], test_X[test_X.columns[4:-1]], test_X[test_X.columns[-1]]\n",
    "\n",
    "transformed_data = scaler.transform(data)\n",
    "test_X = pd.concat([ids, pd.DataFrame(transformed_data, columns = DATA.columns[4:-1]), target], axis=1)\n",
    "X_test_real, y_test_real = test_X.iloc[:, :-1], test_X.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real.drop('target_year',inplace=True,axis=1)\n",
    "X_test_real = X_test_real.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_real_reduced = selector.transform(X_test_real)\n",
    "prediction =  GS.predict(X_test_real)\n",
    "accuracy_score(y_test_real, prediction), recall_score(y_test_real, prediction), \\\n",
    "    precision_score(y_test_real, prediction), f1_score(y_test_real, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
